<!DOCTYPE html>
<html>
  <head>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
    <link rel="shortcut icon" href="images/icon.ico" />

    <style type="text/css">
      body {
        background-color: #f5f9ff;
      }

      math {
        font-family: "STIX Two Math", "Cambria Math", "Latin Modern Math", serif;
        line-height: 1.2;
      }

     
      math[display="block"] {
        display: block;
        margin: 0 auto !important;
        text-align: center !important;
      }

      .mathjax-mobile,
      .mathml-non-mobile {
        display: none;
      }

      .show-mathml .mathml-non-mobile {
        display: block;
      }

      .show-mathjax .mathjax-mobile {
        display: block;
      }

      .content-margin-container {
        display: flex;
        width: 100%;
        justify-content: flex-start;
        align-items: center;
      }

      .main-content-block {
        width: 70%;
        max-width: 1100px;
        background-color: #fff;
        border-left: 1px solid #ddd;
        border-right: 1px solid #ddd;
        padding: 8px;
        font-family: "HelveticaNeue-Light", "Helvetica Neue Light",
          "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
      }

      .margin-left-block {
        font-size: 14px;
        width: 15%;
        max-width: 130px;
        position: relative;
        margin-left: 10px;
        text-align: left;
        font-family: "HelveticaNeue-Light", "Helvetica Neue Light",
          "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
        padding: 5px;
      }

      .margin-right-block {
        font-family: "HelveticaNeue-Light", "Helvetica Neue Light",
          "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
        font-size: 14px;
        width: 25%;
        max-width: 256px;
        position: relative;
        text-align: left;
        padding: 10px;
      }

      img {
        max-width: 100%;
        height: auto;
        display: block;
        margin: auto;
      }

      .equation-container {
        display: flex;
        justify-content: center;
        margin: 8px 0;
      }

      .equation {
      }

      .equation math[display="block"] {
        margin: 0 auto;
      }

      .my-video {
        max-width: 100%;
        height: auto;
        display: block;
        margin: auto;
      }

      .vid-mobile,
      .vid-non-mobile {
        display: none;
      }

      .show-vid-mobile .vid-mobile {
        display: block;
      }

      .show-vid-non-mobile .vid-non-mobile {
        display: block;
      }

      a:link,
      a:visited {
        color: #0e7862;
        text-decoration: none;
      }

      a:hover {
        color: #24b597;
      }

      h1 {
        font-size: 22px;
        margin-top: 4px;
        margin-bottom: 10px;
      }

      h2 {
        font-size: 20px;
        margin-top: 12px;
        margin-bottom: 8px;
      }

      h3 {
        font-size: 16px;
        margin-top: 10px;
        margin-bottom: 6px;
      }

      table.header {
        font-weight: 300;
        font-size: 17px;
        flex-grow: 1;
        width: 70%;
        max-width: calc(100% - 290px);
      }

      table td,
      table td * {
        vertical-align: middle;
        position: relative;
      }

      table.paper-code-tab {
        flex-shrink: 0;
        margin-left: 8px;
        margin-top: 8px;
        padding: 0 0 0 8px;
        width: 290px;
        height: 150px;
      }

      table.booktabs {
        width: 100%;
        border-collapse: collapse;
        border-top: 2px solid #1f2933;
        border-bottom: 2px solid #1f2933;
        margin: 12px 0 20px;
        font-size: 15px;
      }

      table.booktabs caption {
        caption-side: top;
        font-weight: 600;
        text-align: left;
        color: #2c3e50;
        margin-bottom: 6px;
      }

      table.booktabs th,
      table.booktabs td {
        padding: 10px 12px;
      }

      table.booktabs thead th {
        font-weight: 600;
        border-bottom: 1.5px solid #1f2933;
        text-align: left;
      }

      table.booktabs tbody td {
        border-bottom: 1px solid #dfe7f3;
      }

      table.booktabs tbody tr:last-child td {
        border-bottom: none;
      }

      table.booktabs .num {
        text-align: right;
        font-variant-numeric: tabular-nums;
      }

      table.booktabs tbody tr:hover td {
        background-color: #f6f8fc;
      }

      .layered-paper {
        box-shadow: 0px 0px 1px 1px rgba(0, 0, 0, 0.35), 5px 5px 0 0px #fff,
          5px 5px 1px 1px rgba(0, 0, 0, 0.35), 10px 10px 0 0px #fff,
          10px 10px 1px 1px rgba(0, 0, 0, 0.35);
        margin-top: 5px;
        margin-left: 10px;
        margin-right: 30px;
        margin-bottom: 5px;
      }

      hr {
        height: 1px;
        border: none;
        background-color: #ddd;
      }

      div.hypothesis {
        width: 80%;
        background-color: #eee;
        border: 1px solid black;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
        font-family: Courier;
        font-size: 18px;
        text-align: center;
        margin: auto;
        padding: 16px;
      }

      div.citation {
        font-size: 0.8em;
        background-color: #fff;
        padding: 10px;
        height: 200px;
      }

      .fade-in-inline {
        position: absolute;
        text-align: center;
        margin: auto;
        -webkit-mask-image: linear-gradient(
          to right,
          transparent 0%,
          transparent 40%,
          black 50%,
          black 90%,
          transparent 100%
        );
        mask-image: linear-gradient(
          to right,
          transparent 0%,
          transparent 40%,
          black 50%,
          black 90%,
          transparent 100%
        );
        -webkit-mask-size: 8000% 100%;
        mask-size: 8000% 100%;
        animation-name: sweepMask;
        animation-duration: 4s;
        animation-iteration-count: infinite;
        animation-timing-function: linear;
        animation-delay: -1s;
      }

      .fade-in2-inline {
        animation-delay: 1s;
      }

      .inline-div {
        position: relative;
        display: inline-block;
        vertical-align: top;
        width: 50px;
      }

      .model-diagrams {
        display: flex;
        flex-wrap: wrap;
        gap: 16px;
        margin: 16px 0;
      }

      .model-diagrams figure {
        flex: 1 1 200px;
        text-align: center;
        font-size: 14px;
      }

      .model-diagrams figcaption {
        margin-top: 6px;
        color: #555;
      }

      .model-diagrams svg {
        width: 60%;
        max-width: 380px;
        height: auto;
        display: block;
        margin: 0 auto;
      }

      .equation-row {
        display: flex;
        justify-content: center;
        margin: 8px 0;
      }

      .equation-row math[display="block"] {
        margin: 0 auto;
      }
	  .model-diagrams svg {
  width: 48%;   
  max-width: 360px;
  height: auto; 
  display: block;
  margin: 0 auto;
}

    </style>

    <title>Learned Equivariance for Molecular Dipole Prediction</title>

    <meta property="og:title" content="Learned Equivariance for Molecular Dipole Prediction" />
    <meta charset="UTF-8" />
  </head>

  <body>
    <div class="content-margin-container">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <table class="header" align="left">
          <tr>
            <td colspan="4">
              <span
                style="
                  font-size: 32px;
                  font-family: 'Courier New', Courier, monospace;
                "
                >A deeper look into equivariance for molecular data</span
              >
            </td>
          </tr>
          <tr>
            <td align="left">
              <span style="font-size: 17px"
                ><a href="your_website">Timothy Pinkhassik</a></span
              >
            </td>
            <td align="left">
              <span style="font-size: 17px"
                ><a href="your_partner's_website">Almira Nurlanova</a></span
              >
            </td>
          </tr>
          <tr>
            <td colspan="4" align="left">
              <span style="font-size: 18px">Final project for 6.7960, MIT</span>
            </td>
          </tr>
        </table>
      </div>
      <div class="margin-right-block"></div>
    </div>

    <div class="content-margin-container" id="intro">
      <div class="margin-left-block">
        <div style="position: fixed; max-width: inherit; top: max(20%, 120px)">
          <b style="font-size: 16px">Outline</b><br /><br />
          <a href="#intro">Introduction</a><br /><br />
          <a href="#background">Background</a><br /><br />
          <a href="#related_works">Related Works</a><br /><br />
          <a href="#methods">Methods</a><br /><br />
          <a href="#results">Results</a><br /><br />
          <a href="#conclusion">Conclusion</a><br /><br />
          <a href="#implications_and_limitations"
            >Implications and limitations</a
          ><br /><br />
          <a href="#references"
            >References</a
          ><br /><br />
        </div>
      </div>
      <div class="main-content-block">
        <img src="./images/ice_cream.webp" width="512" />
      </div>
      <div class="margin-right-block">Neapolitan ice cream</div>
    </div>

    <div class="content-margin-container" id="intro-main">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h2>Introduction</h2>
        <p>
A molecule's response to light helps verify its synthesis and predict its reactivity. Understanding what this response 
looks like for new molecules, such as a drug candidate, can thus be an extremely powerful tool for chemists. 
Unfortunately, while we have differential equations that can be used to compute its behavior, finding the terms of this 
equation apriori require extensive quantum simulations that are computationally prohibitive for important applications 
such as high-throughput drug screening. In this work, we built on equivariant physics-informed models by creating three
models to predict the 
leading term for this equation, called the dipole. We aimed to show that geometric regularization can steer the latent 
representation toward the correct physical symmetries and improve generalizability, though we uncovered a potential
pitfall when objectives that pull the model towards equivariance can pull it away from accurate prediction.
Our contributions present an important next step in the path towards computationally tractable, 
generalizable, and scalable molecular property prediction.
        </p>
        <p>
          A Graph Neural Network is one of the most suitable
          architectures to represent molecular data, where a node corresponds to
          an atom and an edge corresponds to a chemical bond. Because of this
          structural alignment, GNNs are the primary model architecture used for
          molecular property
          prediction, organic retrosynthesis, and protein-ligand binding.
          However, classic GNNs do not inherently guarantee one of
          the central properties of molecules - rotational and translational
          (SE(3)) equivariance.
        </p>

        <p>
          To address this limitation, a number of architectures with built-in
          equivariance have been designed, such as PaiNN <a href="#ref1">[1]</a>, SchNet <a href="#ref2">[2]</a>, EGNN <a href="#ref3">[3]</a>. Despite
          achieving better performance and more meaningful and accurate physical
          representation, built-in equivariant GNNs are computationally
          expensive, especially when algorithms rely on spherical harmonics or
          per-edge vector updates <a href="#ref4">[4]</a>, <a href="#ref5">[5]</a>.
        </p>

        <p>
          To investigate our
          hypothesis, we designed a set of models to predict the dipole moment of
          small molecules from QM7-X dataset <a href="#ref6">[6]</a>. Dipole moment prediction
          is a particularly suitable benchmark as it is a
          vector quantity; as a molecule gets rotated, the model output should rotate
          in the same exact direction while retaining magnitude. The dipole moment is 
          essential in determining how charge is spread out in a molecule, which 
          defines polarity and influences properties like membrane permeability and 
          solubility.  A high quality model can help avoid running computationally expensive Density 
          Functional Theory (DFT) calculations and is key to predicting how a molecule will absorb infrared light.
       
        </p>

        <p>
          <b>Our central hypothesis</b> is: A non-equivariant GNN trained
          with additional rotational augmentation and a penalty for non-equivariant latent
          representations can approximate SO(3) equivariance, and our latent non-equivariance
          penalty will help push the model towards equivariance approximation faster.
        </p>

        <p>
          To evaluate the effectiveness of equivariance learning we
          compared these three cases:
        </p>

        <ol>
          <li>
            <b>Vanilla</b> ‚Äì a standard, non-equivariant GNN trained on the
            original dataset without rotational augmentation.
          </li>
          <br />
          <li>
            <b>Strawberry</b> ‚Äì a standard, non-equivariant GNN with the same
            architecture as Vanilla, but trained on rotationally augmented data.
            We used Super-Fibonacci sampling to create SO(3) augmentations of our
            train dataset <a href="#ref7">[7]</a>.
          </li>
          <br />
          <li>
            <b>Chocolate</b> ‚Äì our equivariant GNN baseline based on EGNN [CITE EGNN] in which internal
            vector features are guaranteed to rotate with rotations of the
            input.
          </li>
        </ol>
        <div class="figure-wrapper">
                
  <div class="svg-wrapper">
    <svg viewBox="0 0 1400 950" xmlns="http://www.w3.org/2000/svg">
      <text x="700" y="35" font-size="26" font-weight="bold" text-anchor="middle" fill="#2c3e50">
        Strawberry vs Chocolate: Architecture Comparison
      </text>
      <text x="700" y="60" font-size="14" text-anchor="middle" fill="#7f8c8d">
        Non-Equivariant (with augmentation) vs Equivariant GNN for Dipole Prediction
      </text>
      <g id="strawberry">
        <rect x="20" y="90" width="650" height="50" fill="#ffebee" stroke="#c62828" stroke-width="3" rx="5"/>
        <text x="345" y="115" font-size="20" font-weight="bold" text-anchor="middle" fill="#c62828">
          Strawberry (Non-Equivariant)
        </text>
        <text x="345" y="133" font-size="12" text-anchor="middle" fill="#c62828">
          Trained with rotational augmentation
        </text>
        <rect x="120" y="160" width="100" height="50" fill="#e3f2fd" stroke="#1976d2" stroke-width="2" rx="4"/>
        <text x="170" y="182" font-size="12" font-weight="bold" text-anchor="middle" fill="#1565c0">Embedding</text>
        <text x="170" y="198" font-size="10" text-anchor="middle" fill="#1565c0">x: (N, F)</text>
        
        <rect x="270" y="160" width="100" height="50" fill="#e3f2fd" stroke="#1976d2" stroke-width="2" rx="4"/>
        <text x="320" y="182" font-size="12" font-weight="bold" text-anchor="middle" fill="#1565c0">Expand pos</text>
        <text x="320" y="198" font-size="10" text-anchor="middle" fill="#1565c0">v: (N, 3, F)</text>
        
        <line x1="170" y1="210" x2="170" y2="240" stroke="#2c3e50" stroke-width="2" marker-end="url(#arrowhead)"/>
        <line x1="320" y1="210" x2="320" y2="240" stroke="#2c3e50" stroke-width="2" marker-end="url(#arrowhead)"/>
        
        <rect x="45" y="240" width="590" height="450" fill="#fff8f0" stroke="#e65100" stroke-width="3" rx="6"/>
        <text x="340" y="265" font-size="16" font-weight="bold" text-anchor="middle" fill="#e65100">
          Strawberry Layer (√ó3)
        </text>
        
        <rect x="100" y="285" width="240" height="55" fill="#fff3e0" stroke="#f57c00" stroke-width="2" rx="4"/>
        <text x="220" y="305" font-size="13" font-weight="bold" text-anchor="middle" fill="#e65100">1. Feature Fusion</text>
        <text x="220" y="322" font-size="11" text-anchor="middle" fill="#e65100">node_feats = [x, v_flat]</text>
        <text x="220" y="335" font-size="10" text-anchor="middle" fill="#7f8c8d">(N, 4F)</text>
        
        <rect x="360" y="285" width="240" height="55" fill="#e8f5e9" stroke="#388e3c" stroke-width="2" rx="4"/>
        <text x="480" y="305" font-size="13" font-weight="bold" text-anchor="middle" fill="#2e7d32">2. Compute Geometry</text>
        <text x="480" y="322" font-size="11" text-anchor="middle" fill="#2e7d32">pseudo_pos = v.mean(dim=-1)</text>
        <text x="480" y="335" font-size="10" text-anchor="middle" fill="#7f8c8d">dist_ij from pseudo positions</text>
        
        <line x1="220" y1="340" x2="220" y2="365" stroke="#666" stroke-width="1.5" marker-end="url(#arrowhead)"/>
        <line x1="480" y1="340" x2="380" y2="365" stroke="#666" stroke-width="1.5" marker-end="url(#arrowhead)"/>
        
        <rect x="155" y="365" width="370" height="70" fill="#fce4ec" stroke="#c2185b" stroke-width="2" rx="4"/>
        <text x="340" y="388" font-size="14" font-weight="bold" text-anchor="middle" fill="#880e4f">3. Non-Equivariant Message MLP</text>
        <text x="340" y="406" font-size="11" text-anchor="middle" fill="#880e4f">Input: [node_feats[i], node_feats[j], dist_ij]</text>
        <text x="340" y="422" font-size="10" text-anchor="middle" fill="#7f8c8d">(8F + 1) ‚Üí (4F) unified message</text>
        
        <line x1="270" y1="435" x2="220" y2="460" stroke="#666" stroke-width="1.5" marker-end="url(#arrowhead)"/>
        <line x1="410" y1="435" x2="460" y2="460" stroke="#666" stroke-width="1.5" marker-end="url(#arrowhead)"/>
        
        <rect x="120" y="460" width="200" height="55" fill="#e1f5fe" stroke="#0288d1" stroke-width="2" rx="4"/>
        <text x="220" y="480" font-size="12" font-weight="bold" text-anchor="middle" fill="#01579b">4. Split: Scalar msg</text>
        <text x="220" y="497" font-size="10" text-anchor="middle" fill="#7f8c8d">msg_scalar (E, F)</text>
        <text x="220" y="509" font-size="9" text-anchor="middle" fill="#7f8c8d">first F dimensions</text>
        
        <rect x="360" y="460" width="200" height="55" fill="#e1f5fe" stroke="#0288d1" stroke-width="2" rx="4"/>
        <text x="460" y="480" font-size="12" font-weight="bold" text-anchor="middle" fill="#01579b">4. Split: Vector msg</text>
        <text x="460" y="497" font-size="10" text-anchor="middle" fill="#7f8c8d">msg_vector (E, 3, F)</text>
        <text x="460" y="509" font-size="9" text-anchor="middle" fill="#7f8c8d">last 3F dimensions</text>
        
        <line x1="220" y1="515" x2="220" y2="540" stroke="#666" stroke-width="1.5" marker-end="url(#arrowhead)"/>
        <line x1="460" y1="515" x2="460" y2="540" stroke="#666" stroke-width="1.5" marker-end="url(#arrowhead)"/>
        
        <rect x="120" y="540" width="200" height="45" fill="#f3e5f5" stroke="#7b1fa2" stroke-width="2" rx="4"/>
        <text x="220" y="558" font-size="12" font-weight="bold" text-anchor="middle" fill="#4a148c">5. Aggregate</text>
        <text x="220" y="575" font-size="10" text-anchor="middle" fill="#7f8c8d">agg_s: (N, F)</text>
        
        <rect x="360" y="540" width="200" height="45" fill="#f3e5f5" stroke="#7b1fa2" stroke-width="2" rx="4"/>
        <text x="460" y="558" font-size="12" font-weight="bold" text-anchor="middle" fill="#4a148c">5. Aggregate</text>
        <text x="460" y="575" font-size="10" text-anchor="middle" fill="#7f8c8d">agg_v: (N, 3, F)</text>
        
        <line x1="220" y1="585" x2="220" y2="610" stroke="#666" stroke-width="1.5" marker-end="url(#arrowhead)"/>
        <line x1="460" y1="585" x2="460" y2="610" stroke="#666" stroke-width="1.5" marker-end="url(#arrowhead)"/>
        
        <rect x="120" y="610" width="200" height="50" fill="#c8e6c9" stroke="#2e7d32" stroke-width="2" rx="4"/>
        <text x="220" y="630" font-size="12" font-weight="bold" text-anchor="middle" fill="#1b5e20">6. Update Scalar</text>
        <text x="220" y="647" font-size="10" text-anchor="middle" fill="#1b5e20">x + MLP([x, agg_s])</text>
        
        <rect x="360" y="610" width="200" height="50" fill="#c8e6c9" stroke="#2e7d32" stroke-width="2" rx="4"/>
        <text x="460" y="630" font-size="12" font-weight="bold" text-anchor="middle" fill="#1b5e20">6. Update Vector</text>
        <text x="460" y="647" font-size="10" text-anchor="middle" fill="#1b5e20">v + agg_v + v_mix</text>
        
        <ellipse cx="220" cy="675" rx="35" ry="20" fill="#4caf50" stroke="#2e7d32" stroke-width="2"/>
        <text x="220" y="680" font-size="11" font-weight="bold" text-anchor="middle" fill="white">x_new</text>
        
        <ellipse cx="460" cy="675" rx="35" ry="20" fill="#4caf50" stroke="#2e7d32" stroke-width="2"/>
        <text x="460" y="680" font-size="11" font-weight="bold" text-anchor="middle" fill="white">v_new</text>
        
        <line x1="495" y1="675" x2="580" y2="675" stroke="#9c27b0" stroke-width="2" marker-end="url(#arrowhead)"/>
        <rect x="580" y="655" width="45" height="40" fill="#f3e5f5" stroke="#9c27b0" stroke-width="2" rx="4"/>
        <text x="602" y="670" font-size="9" font-weight="bold" text-anchor="middle" fill="#6a1b9a">Layer</text>
        <text x="602" y="682" font-size="9" text-anchor="middle" fill="#6a1b9a">Loss</text>
        
        <line x1="460" y1="695" x2="460" y2="715" stroke="#2c3e50" stroke-width="2" marker-end="url(#arrowhead)"/>
        
        <rect x="360" y="715" width="200" height="50" fill="#bbdefb" stroke="#1976d2" stroke-width="2" rx="4"/>
        <text x="460" y="735" font-size="12" font-weight="bold" text-anchor="middle" fill="#0d47a1">Global Pooling (v only)</text>
        <text x="460" y="752" font-size="10" text-anchor="middle" fill="#0d47a1">Sum vectors ‚Üí (B, 3, F)</text>
        
        <line x1="460" y1="765" x2="460" y2="785" stroke="#2c3e50" stroke-width="2" marker-end="url(#arrowhead)"/>
        
        <rect x="360" y="785" width="200" height="50" fill="#81c784" stroke="#388e3c" stroke-width="2" rx="4"/>
        <text x="460" y="805" font-size="12" font-weight="bold" text-anchor="middle" fill="#1b5e20">Linear Head</text>
        <text x="460" y="822" font-size="10" text-anchor="middle" fill="#1b5e20">Dipole: (B, 3)</text>
      </g>
      
      <g id="chocolate">
        <rect x="730" y="90" width="650" height="50" fill="#e8eaf6" stroke="#3f51b5" stroke-width="3" rx="5"/>
        <text x="1055" y="115" font-size="20" font-weight="bold" text-anchor="middle" fill="#3f51b5">
          Chocolate (Equivariant)
        </text>
        <text x="1055" y="133" font-size="12" text-anchor="middle" fill="#3f51b5">
          Rotation-invariant by design
        </text>
        
        <rect x="855" y="160" width="100" height="50" fill="#e3f2fd" stroke="#1976d2" stroke-width="2" rx="4"/>
        <text x="905" y="182" font-size="12" font-weight="bold" text-anchor="middle" fill="#1565c0">Embedding</text>
        <text x="905" y="198" font-size="10" text-anchor="middle" fill="#1565c0">x: (N, F)</text>
        
        <rect x="1005" y="160" width="100" height="50" fill="#e3f2fd" stroke="#1976d2" stroke-width="2" rx="4"/>
        <text x="1055" y="182" font-size="12" font-weight="bold" text-anchor="middle" fill="#1565c0">Expand pos</text>
        <text x="1055" y="198" font-size="10" text-anchor="middle" fill="#1565c0">v: (N, 3, F)</text>
        
        <line x1="905" y1="210" x2="905" y2="240" stroke="#2c3e50" stroke-width="2" marker-end="url(#arrowhead)"/>
        <line x1="1055" y1="210" x2="1055" y2="240" stroke="#2c3e50" stroke-width="2" marker-end="url(#arrowhead)"/>
        
        <rect x="755" y="240" width="590" height="450" fill="#f0f4ff" stroke="#3949ab" stroke-width="3" rx="6"/>
        <text x="1050" y="265" font-size="16" font-weight="bold" text-anchor="middle" fill="#3949ab">
          Chocolate Layer (√ó3)
        </text>
        
        <rect x="810" y="285" width="240" height="70" fill="#e8f5e9" stroke="#388e3c" stroke-width="2" rx="4"/>
        <text x="930" y="305" font-size="13" font-weight="bold" text-anchor="middle" fill="#2e7d32">1. Compute Geometry</text>
        <text x="930" y="320" font-size="11" text-anchor="middle" fill="#2e7d32">r_ij = pos[j] - pos[i]</text>
        <text x="930" y="333" font-size="11" text-anchor="middle" fill="#2e7d32">dir_ij = r_ij / ||r_ij||</text>
        <text x="930" y="345" font-size="10" text-anchor="middle" fill="#7f8c8d">dist_ij = ||r_ij||</text>
        <line x1="930" y1="355" x2="1050" y2="365"
      stroke="#2c3e50" stroke-width="2" 
      marker-end="url(#arrowhead)" />
        
        <rect x="1070" y="285" width="240" height="55" fill="#fff3e0" stroke="#f57c00" stroke-width="2" rx="4"/>
        <text x="1190" y="305" font-size="13" font-weight="bold" text-anchor="middle" fill="#e65100">2. Scalar-Only MLP</text>
        <text x="1190" y="322" font-size="11" text-anchor="middle" fill="#e65100">Input: [x[i], x[j], dist_ij]</text>
        <text x="1190" y="335" font-size="10" text-anchor="middle" fill="#7f8c8d">(2F + 1) ‚Üí (2F)</text>
        
        <line x1="1190" y1="340" x2="1090" y2="365" stroke="#666" stroke-width="1.5" marker-end="url(#arrowhead)"/>
        
        <rect x="925" y="365" width="250" height="70" fill="#fce4ec" stroke="#c2185b" stroke-width="2" rx="4"/>
        <text x="1050" y="388" font-size="14" font-weight="bold" text-anchor="middle" fill="#880e4f">3. Split Message</text>
        <text x="1050" y="406" font-size="11" text-anchor="middle" fill="#880e4f">gate_vec (E, F) | msg_scalar (E, F)</text>
        <text x="1050" y="422" font-size="10" text-anchor="middle" fill="#7f8c8d">Both from scalar MLP output</text>
        
        <line x1="975" y1="435" x2="930" y2="460" stroke="#666" stroke-width="1.5" marker-end="url(#arrowhead)"/>
        <line x1="1125" y1="435" x2="1170" y2="460" stroke="#666" stroke-width="1.5" marker-end="url(#arrowhead)"/>
        
        <rect x="810" y="460" width="240" height="60" fill="#e1f5fe" stroke="#0288d1" stroke-width="2" rx="4"/>
        <text x="930" y="480" font-size="12" font-weight="bold" text-anchor="middle" fill="#01579b">4. Equivariant Vector Message</text>
        <text x="930" y="497" font-size="10" text-anchor="middle" fill="#7f8c8d">m_vec = r_hat_ij ‚äó gate_vec</text>
        <text x="930" y="510" font-size="9" text-anchor="middle" fill="#7f8c8d">Uses unit dir_ij to gate vectors</text>
        
        <rect x="1070" y="460" width="240" height="55" fill="#e1f5fe" stroke="#0288d1" stroke-width="2" rx="4"/>
        <text x="1190" y="480" font-size="12" font-weight="bold" text-anchor="middle" fill="#01579b">4. Scalar Message</text>
        <text x="1190" y="497" font-size="10" text-anchor="middle" fill="#7f8c8d">msg_scalar (E, F)</text>
        <text x="1190" y="509" font-size="9" text-anchor="middle" fill="#7f8c8d">Direct from MLP</text>
        
        <line x1="930" y1="515" x2="930" y2="540" stroke="#666" stroke-width="1.5" marker-end="url(#arrowhead)"/>
        <line x1="1190" y1="515" x2="1190" y2="540" stroke="#666" stroke-width="1.5" marker-end="url(#arrowhead)"/>
        
        <rect x="810" y="540" width="240" height="45" fill="#f3e5f5" stroke="#7b1fa2" stroke-width="2" rx="4"/>
        <text x="930" y="558" font-size="12" font-weight="bold" text-anchor="middle" fill="#4a148c">5. Aggregate</text>
        <text x="930" y="575" font-size="10" text-anchor="middle" fill="#7f8c8d">agg_v: (N, 3, F)</text>
        
        <rect x="1070" y="540" width="240" height="45" fill="#f3e5f5" stroke="#7b1fa2" stroke-width="2" rx="4"/>
        <text x="1190" y="558" font-size="12" font-weight="bold" text-anchor="middle" fill="#4a148c">5. Aggregate</text>
        <text x="1190" y="575" font-size="10" text-anchor="middle" fill="#7f8c8d">agg_s: (N, F)</text>
        
        <line x1="930" y1="585" x2="930" y2="610" stroke="#666" stroke-width="1.5" marker-end="url(#arrowhead)"/>
        <line x1="1190" y1="585" x2="1190" y2="610" stroke="#666" stroke-width="1.5" marker-end="url(#arrowhead)"/>
        
        <rect x="810" y="610" width="240" height="50" fill="#c8e6c9" stroke="#2e7d32" stroke-width="2" rx="4"/>
        <text x="930" y="630" font-size="12" font-weight="bold" text-anchor="middle" fill="#1b5e20">6. Update Vector</text>
        <text x="930" y="647" font-size="10" text-anchor="middle" fill="#1b5e20">v + agg_v + v_mix</text>
        
        <rect x="1070" y="610" width="240" height="50" fill="#c8e6c9" stroke="#2e7d32" stroke-width="2" rx="4"/>
        <text x="1190" y="630" font-size="12" font-weight="bold" text-anchor="middle" fill="#1b5e20">6. Update Scalar</text>
        <text x="1190" y="647" font-size="10" text-anchor="middle" fill="#1b5e20">x + MLP([x, agg_s])</text>
        
        <ellipse cx="930" cy="675" rx="35" ry="20" fill="#4caf50" stroke="#2e7d32" stroke-width="2"/>
        <text x="930" y="680" font-size="11" font-weight="bold" text-anchor="middle" fill="white">v_new</text>
        
        <ellipse cx="1190" cy="675" rx="35" ry="20" fill="#4caf50" stroke="#2e7d32" stroke-width="2"/>
        <text x="1190" y="680" font-size="11" font-weight="bold" text-anchor="middle" fill="white">x_new</text>
        
        <line x1="930" y1="695" x2="930" y2="715" stroke="#2c3e50" stroke-width="2" marker-end="url(#arrowhead)"/>
        
        <rect x="830" y="715" width="200" height="50" fill="#bbdefb" stroke="#1976d2" stroke-width="2" rx="4"/>
        <text x="930" y="735" font-size="12" font-weight="bold" text-anchor="middle" fill="#0d47a1">Global Pooling (v only)</text>
        <text x="930" y="752" font-size="10" text-anchor="middle" fill="#0d47a1">Sum vectors ‚Üí (B, 3, F)</text>
        
        <line x1="930" y1="765" x2="930" y2="785" stroke="#2c3e50" stroke-width="2" marker-end="url(#arrowhead)"/>
        
        <rect x="830" y="785" width="200" height="50" fill="#81c784" stroke="#388e3c" stroke-width="2" rx="4"/>
        <text x="930" y="805" font-size="12" font-weight="bold" text-anchor="middle" fill="#1b5e20">Linear Head</text>
        <text x="930" y="822" font-size="10" text-anchor="middle" fill="#1b5e20">Dipole: (B, 3)</text>
      </g>
      <defs>
        <marker id="arrowhead" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
          <polygon points="0 0, 10 3, 0 6" fill="#2c3e50"/>
        </marker>
      </defs>
    </svg>
  </div>
</div>
        <hr />
        <a id="background"></a>
        <h2>Background</h2>

        <h3>Equivariance</h3>
        <p>
          Let's define invariance and equivariance. Suppose we have a function
          <em>f</em> and a transformation <em>g</em>. Then <em>f</em> is
          invariant to <em>g</em> if
        </p>

        <div class="equation-container">
          <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
            <mrow>
              <mi>f</mi>
              <mo>(</mo>
              <mi>g</mi>
              <mo>‚ãÖ</mo>
              <mi>x</mi>
              <mo>)</mo>
              <mo>=</mo>
              <mi>f</mi>
              <mo>(</mo>
              <mi>x</mi>
              <mo>)</mo>
            </mrow>
          </math>
        </div>

        <p>and <em>f</em> is equivariant to <em>g</em> if</p>

        <div class="equation-container">
          <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
            <mrow>
              <mi>f</mi>
              <mo>(</mo>
              <mi>g</mi>
              <mo>‚ãÖ</mo>
              <mi>x</mi>
              <mo>)</mo>
              <mo>=</mo>
              <mi>g</mi>
              <mo>‚ãÖ</mo>
              <mi>f</mi>
              <mo>(</mo>
              <mi>x</mi>
              <mo>)</mo>
            </mrow>
          </math>
        </div>

        <p>
          In our case, we treat a molecule as a tuple
          <math xmlns="http://www.w3.org/1998/Math/MathML">
            <mi mathvariant="bold">M</mi>
          </math>
          :
        </p>
        <div class="equation-container">
          <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
            <mrow>
              <mi mathvariant="bold">M</mi>
              <mo>=</mo>
              <mo>(</mo>
              <mi>V</mi>
              <mo>,</mo>
              <mi>E</mi>
              <mo>,</mo>
              <mi>z</mi>
              <mo>,</mo>
              <mi mathvariant="bold">r</mi>
              <mo>,</mo>
              <mi mathvariant="bold">Œº</mi>
              <mo>)</mo>
            </mrow>
          </math>
        </div>

        <p>
          Here,
          <math xmlns="http://www.w3.org/1998/Math/MathML">
            <mi>V</mi>
          </math>
          is the set of nodes (atoms),
          <math xmlns="http://www.w3.org/1998/Math/MathML">
            <mi>E</mi>
          </math>
          is the set of edges (we treat the molecule as a point cloud so there
          are
          <math xmlns="http://www.w3.org/1998/Math/MathML">
            <mrow>
              <msup>
                <mi>N</mi>
                <mn>2</mn>
              </msup>
              <mo>‚àí</mo>
              <mi>N</mi>
            </mrow>
          </math>
          directed edges),
          <math xmlns="http://www.w3.org/1998/Math/MathML">
            <msub>
              <mi>z</mi>
              <mi>i</mi>
            </msub>
          </math>
          are node labels (atomic identities),
          <math xmlns="http://www.w3.org/1998/Math/MathML">
            <mrow>
              <mi mathvariant="bold">r</mi>
              <mo>=</mo>
              <mo>{</mo>
              <msub>
                <mi>r</mi>
                <mi>i</mi>
              </msub>
              <mo>}</mo>
            </mrow>
          </math>
          are 3D coordinates of atoms, and
          <math xmlns="http://www.w3.org/1998/Math/MathML">
            <mi mathvariant="bold">Œº</mi>
          </math>
          is the dipole moment vector.
        </p>
        <p>
          We used a rigid-body transformation consisting of a rotation (a
          zero-vector as translational component), therefore our tuple could be
          expressed as follows after rotation:
        </p>
        <div class="equation-container">
          <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
            <mrow>
              <mi>R</mi>
              <mo>‚ãÖ</mo>
              <mi mathvariant="bold">M</mi>
              <mo>=</mo>
              <mo>(</mo>
              <mi>V</mi>
              <mo>,</mo>
              <mi>E</mi>
              <mo>,</mo>
              <mi>z</mi>
              <mo>,</mo>
              <mi>R</mi>
              <mi mathvariant="bold">r</mi>
              <mo>,</mo>
              <mi>R</mi>
              <mi mathvariant="bold">Œº</mi>
              <mo>)</mo>
            </mrow>
          </math>
        </div>
        <p>
          Keep in mind that since 3D coordinates and dipole moment are 3D vector
          quantities, we need to make sure that ground truth dipole moment gets
          rotated as well as the input molecule as you can see above.
        </p>
      </div>
      <div class="margin-right-block"></div>
    </div>
    <div class="content-margin-container" id="related_works">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h2>Related Works</h2>
        <p>
          There have been many attempts to learn the best representation of
          molecular data for better convergence, including string-based
          approaches (e.g. SMILES <a href="#ref8">[8]</a>) and fingerprint-based approaches (e.g. Extended-Connectivity Fingerprints <a href="#ref9">[9]</a>). However, these methods tend to lose structural
          information of molecules, which limits number of tasks neural networks
          could achieve with molecular data. This limitation drove the
          improvement of GNN-based models that operate directly on atomic graphs
          embedded in 3D space.
        </p>
        <h3>Rotational and Translational Invariance based GNN models</h3>

        <h4>SchNet <a href="#ref1">[1]</a> </h4>
        <p>
          This deep learning architecture uses continuous-filter convolution
          layers, rather than message passing algorithms. Continuous-filter
          convolution layers make it feasible to work with molecules scattered
          in 3D space, rather than classic grid view layers. The filter is
          implemented as a neural network, therefore it is a function, not a
          fixed kernel, that takes a displacement vector and outputs a learned
          weight. For efficiency the operation is applied per feature channel.
        </p>
        <p>
          However, SchNet cannot be used to predict vector quantities, as it
          only guarantees translational and rotational invariance, but not
          equivariance.
        </p>

        <h4>Equivariant Graph Neural Network (EGNN) <a href="#ref3">[3]</a></h4>
        <p>
          This deep learning architecture solves the SE(3) equivariance problem,
          while additionally introducing reflection equivariance, which makes
          the architecture E(n) equivariant. It involves more extended message
          passing that includes both scalar and vector features, turning the
          architecture suitable to predict dipole moment as well. Although the
          algorithm does not rely on spherical harmonics, it is more expressive
          than invariant SchNet, which comes with higher computational cost due
          to computing vector updates per edge.
        </p>

        <h4>Polarizable Interaction Neural Network (PaiNN) <a href="#ref2">[2]</a></h4>
        <p>
          It is a tensor-based SE(3)-equivariant network designed as a successor
          of SchNet. PaiNN updates both scalar and vector channels, which allows predicting vector-valued 
          quantities, including dipole moment. PaiNN does not rely on spherical harmonics, but it uses interaction 
          blocks that mix scalar and vector features that preserve SE(3) transformation rules. However, it is also
          computationally expensive due to the need to per-edge vector updates.
        </p>

        <p>
          While mentioned invariant (or equivariant) architectures achieve high
          performance in vector quantity prediction, there is limited analysis
          and comparison of built-in equivariance and learned equivariance using
          rotational augmentation. Our project addresses this gap, which would
          allow to achieve similar performance at lower computational cost,
          while requiring the same amount of data. Superfibonacci sampling
          technique that was used to obtain more data through augmentation is a
          cheap process that does not significantly add up to cost.
        </p>
      </div>
      <div class="margin-right-block">Another method <a href="#ref_10">[10]</a> was published recently, where authors 
        used GNN architecture with SO(3) equivariance embedded through local coordinates to predict the tensorial 
        response properties of molecules, including dipole moment. </div>
    </div>

    <div class="content-margin-container" id="methods">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Methods</h1>

        <p>
          For a simple prediction task on molecular data sensitive to 3D
          geometry, we chose to predict the vector molecular dipole moment for
          small molecules from the QM7-x dataset. This
          dataset contains approximately 7,000 small molecules with up to 7
          "heavy" (non-hydrogen) atoms each, in various conformations. We
          treated each molecule as a point cloud with labeled atom types. The
          coordinates of molecules in QM7x are centered, as are coordinates of
          other computational datasets. For computational tractability, we
          restricted our analysis to SO(3) equivariance instead of the more
          popular E(3) objective, which avoided the necessity of augmenting data
          with translations and simplified evaluation. We expect the
          conclusions found from an analysis of SO(3) equivariance to be
          transferrable to other symmetries.
        </p>

        <h3>Model Architecture</h3>

        <p>
          For each structure with
          <math xmlns="http://www.w3.org/1998/Math/MathML">
            <mi>N</mi>
          </math>
          atoms, we built all
          <math xmlns="http://www.w3.org/1998/Math/MathML">
            <mrow>
              <msup>
                <mi>N</mi>
                <mn>2</mn>
              </msup>
              <mo>‚àí</mo>
              <mi>N</mi>
            </mrow>
          </math>
          directed edges
          <math xmlns="http://www.w3.org/1998/Math/MathML">
            <mrow>
              <mo>(</mo>
              <mi>i</mi>
              <mo>,</mo>
              <mi>j</mi>
              <mo>)</mo>
            </mrow>
          </math>
          where
          <math xmlns="http://www.w3.org/1998/Math/MathML">
            <mrow>
              <mi>i</mi>
              <mo>‚â†</mo>
              <mi>j</mi>
            </mrow>
          </math>
          . Each node was featurized with a learned scalar embedding of the atom
          type from the set (H, C, N, O, F, P, S, Cl). Each edge
          <math xmlns="http://www.w3.org/1998/Math/MathML">
            <mrow>
              <mo>(</mo>
              <mi>i</mi>
              <mo>,</mo>
              <mi>j</mi>
              <mo>)</mo>
            </mrow>
          </math>
          was featurized with the distance between atoms
          <math xmlns="http://www.w3.org/1998/Math/MathML">
            <mi>i</mi>
          </math>
          and
          <math xmlns="http://www.w3.org/1998/Math/MathML">
            <mi>j</mi>
          </math>
          . We split the dataset into training, validation, and test sets with
          80%, 10%, and 10% of molecules, respectively, ensuring that different
          conformations of the same molecule were all in the same split. For
          tractability in training, we randomly downsampled the training set to
          200 conformers per molecule.
        </p>

        <p>A single layer of Chocolate maps</p>

        <div class="equation-container">
          <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
            <mrow>
              <mo>(</mo>
              <msup>
                <mi>x</mi>
                <mrow>
                  <mo>(</mo>
                  <mi>l</mi>
                  <mo>)</mo>
                </mrow>
              </msup>
              <mo>,</mo>
              <mi mathvariant="bold">v</mi>
              <mo>)</mo>
              <mo>‚Ü¶</mo>
              <mo>(</mo>
              <msup>
                <mi>x</mi>
                <mrow>
                  <mo>(</mo>
                  <mi>l</mi>
                  <mo>+</mo>
                  <mn>1</mn>
                  <mo>)</mo>
                </mrow>
              </msup>
              <mo>,</mo>
              <mi mathvariant="bold">v</mi>
              <mo>)</mo>
            </mrow>
          </math>
        </div>

        <p>
          Given a node
          <math xmlns="http://www.w3.org/1998/Math/MathML">
            <mi>i</mi>
          </math>
          with adjacent node
          <math xmlns="http://www.w3.org/1998/Math/MathML">
            <mi>j</mi>
          </math>
          , the layer computes:
        </p>

        <ol>
          <li>The direction of the neighboring node and its distance:</li>
          <br />
          <div class="equation-container">
            <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
              <mrow>
                <msub>
                  <mi>ùê´</mi>
                  <mi>ij</mi>
                </msub>
                <mo>=</mo>
                <msub>
                  <mi>ùê´</mi>
                  <mi>j</mi>
                </msub>
                <mo>‚àí</mo>
                <msub>
                  <mi>ùê´</mi>
                  <mi>i</mi>
                </msub>
              </mrow>
            </math>
          </div>
          <br />
          <div class="equation-container">
            <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
              <mrow>
                <msub>
                  <mi>d</mi>
                  <mi>ij</mi>
                </msub>
                <mo>=</mo>
                <msup>
                  <mrow>
                    <mo>‚Äñ</mo>
                    <msub>
                      <mi>ùê´</mi>
                      <mi>ij</mi>
                    </msub>
                    <mo>‚Äñ</mo>
                  </mrow>
                  <mn>2</mn>
                </msup>
              </mrow>
            </math>
          </div>
          <br />
          <div class="equation-container">
            <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
              <mrow>
                <msub>
                  <mover>
                    <mi>ùê´</mi>
                    <mo>^</mo>
                  </mover>
                  <mi>ij</mi>
                </msub>
                <mo>=</mo>
                <mfrac>
                  <msub>
                    <mi>ùê´</mi>
                    <mi>ij</mi>
                  </msub>
                  <msub>
                    <mi>d</mi>
                    <mi>ij</mi>
                  </msub>
                </mfrac>
              </mrow>
            </math>
          </div>
          <br />

          <li>Concatenates scalar features to form message input:</li>

          <br />

          <div class="equation-container">
            <math
              display="block"
              xmlns="http://www.w3.org/1998/Math/MathML"
            >
              <mrow>
                <msub>
                  <mi>m</mi>
                  <mi>ij</mi>
                </msub>
                <mo>=</mo>
                <msub>
                  <mi>MLP</mi>
                  <mi>msg</mi>
                </msub>
                <mo>(</mo>
                <msubsup>
                  <mi>x</mi>
                  <mi>i</mi>
                  <mrow>
                    <mo>(</mo>
                    <mi>l</mi>
                    <mo>)</mo>
                  </mrow>
                </msubsup>
                <mo>,</mo>
                <msubsup>
                  <mi>x</mi>
                  <mi>j</mi>
                  <mrow>
                    <mo>(</mo>
                    <mi>l</mi>
                    <mo>)</mo>
                  </mrow>
                </msubsup>
                <mo>,</mo>
                <msub>
                  <mi>d</mi>
                  <mi>ij</mi>
                </msub>
                <mo>)</mo>
              </mrow>
            </math>
          </div>
          <br />

          <li>
            Splits the output of the message MLP into vector gates and scalar
            messages:
          </li>

          <br />

          <div class="equation-container">
            <math
              display="block"
              xmlns="http://www.w3.org/1998/Math/MathML"
            >
              <mrow>
                <mo>(</mo>
                <msub>
                  <mi>g</mi>
                  <mi>ij</mi>
                </msub>
                <mo>,</mo>
                <msub>
                  <mi>s</mi>
                  <mi>ij</mi>
                </msub>
                <mo>)</mo>
                <mo>=</mo>
                <mo>split</mo>
                <mo>(</mo>
                <msub>
                  <mi>m</mi>
                  <mi>ij</mi>
                </msub>
                <mo>)</mo>
              </mrow>
            </math>
          </div>
          <br />

          <li>Calculates the vector message:</li>

          <br />

          <div class="equation-container">
            <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
              <mrow>
                <msubsup>
                  <mi>m</mi>
                  <mi>ij</mi>
                  <mtext>vec</mtext>
                </msubsup>
                <mo>=</mo>
                <msub>
                  <mover>
                    <mi>ùê´</mi>
                    <mo>^</mo>
                  </mover>
                  <mi>ij</mi>
                </msub>
                <mo>‚äó</mo>
                <msub>
                  <mi>g</mi>
                  <mi>ij</mi>
                </msub>
              </mrow>
            </math>
          </div>
          <br />

          <li>Aggregates the messages:</li>

          <br />

          <div class="equation-container">
            <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
              <mrow>
                <msub>
                  <mi>agg</mi>
                  <mi>x</mi>
                </msub>
                <mo stretchy="false">(</mo>
                <mi>i</mi>
                <mo stretchy="false">)</mo>
                <mo>=</mo>
                <munder>
                  <mo movablelimits="false">‚àë</mo>
                  <mrow>
                    <mi>j</mi>
                    <mo>‚â†</mo>
                    <mi>i</mi>
                  </mrow>
                </munder>
                <msub>
                  <mi>s</mi>
                  <mi>ij</mi>
                </msub>
              </mrow>
            </math>
          </div>
          <br />
          <div class="equation-container">
            <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
              <mrow>
                <msub>
                  <mi>agg</mi>
                  <mi>ùêØ</mi>
                </msub>
                <mo stretchy="false">(</mo>
                <mi>i</mi>
                <mo stretchy="false">)</mo>
                <mo>=</mo>
                <munder>
                  <mo movablelimits="false">‚àë</mo>
                  <mrow>
                    <mi>j</mi>
                    <mo>‚â†</mo>
                    <mi>i</mi>
                  </mrow>
                </munder>
                <msub>
                  <mi>m</mi>
                  <mi>ij</mi>
                </msub>
              </mrow>
            </math>
          </div>
          <br />

          <li>
            Mixes the hidden channels of the vector features (applies a single
            linear layer):
          </li>

          <br />

            <div class="equation-container">
              <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
                <mrow>
                  <msubsup>
                    <mi>ùêØ</mi>
                    <mi>i</mi>
                    <mtext>mix</mtext>
                  </msubsup>
                  <mo>=</mo>
                  <msubsup>
                    <mi>ùêØ</mi>
                    <mi>i</mi>
                    <mrow>
                      <mo>(</mo>
                      <mi>l</mi>
                      <mo>)</mo>
                    </mrow>
                  </msubsup>
                  <msub>
                    <mi>W</mi>
                    <mtext>mix</mtext>
                  </msub>
                </mrow>
              </math>
            </div>

          <li>And finally applies the updates:</li>

          <br />

          <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
            <mrow>
              <msubsup>
                <mi>x</mi>
                <mi>i</mi>
                <mrow>
                  <mo>(</mo>
                  <mi>l</mi>
                  <mo>+</mo>
                  <mn>1</mn>
                  <mo>)</mo>
                </mrow>
              </msubsup>
              <mo>=</mo>
              <msubsup>
                <mi>x</mi>
                <mi>i</mi>
                <mrow>
                  <mo>(</mo>
                  <mi>l</mi>
                  <mo>)</mo>
                </mrow>
              </msubsup>
              <mo>+</mo>
              <msub>
                <mi>MLP</mi>
                <mtext>upd</mtext>
              </msub>
              <mo>(</mo>
              <msubsup>
                <mi>x</mi>
                <mi>i</mi>
                <mrow>
                  <mo>(</mo>
                  <mi>l</mi>
                  <mo>)</mo>
                </mrow>
              </msubsup>
              <mo>,</mo>
              <msub>
                <mi>agg</mi>
                <mi>x</mi>
              </msub>
              <mo>(</mo>
              <mi>i</mi>
              <mo>)</mo>
              <mo>)</mo>
            </mrow>
          </math>
          <br />
          <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
            <mrow>
              <msubsup>
                <mi>ùêØ</mi>
                <mi>i</mi>
                <mrow>
                  <mo>(</mo>
                  <mi>l</mi>
                  <mo>+</mo>
                  <mn>1</mn>
                  <mo>)</mo>
                </mrow>
              </msubsup>
              <mo>=</mo>
              <msubsup>
                <mi>ùêØ</mi>
                <mi>i</mi>
                <mrow>
                  <mo>(</mo>
                  <mi>l</mi>
                  <mo>)</mo>
                </mrow>
              </msubsup>
              <mo>+</mo>
              <msub>
                <mi>agg</mi>
                <mi>ùêØ</mi>
              </msub>
              <mo>(</mo>
              <mi>i</mi>
              <mo>)</mo>
              <mo>+</mo>
              <msubsup>
                <mi>ùêØ</mi>
                <mi>i</mi>
                <mtext>mix</mtext>
              </msubsup>
            </mrow>
          </math>
        </ol>

        <p>
          The equivariance of this model is guaranteed by the use of relative
          position vectors
          <math xmlns="http://www.w3.org/1998/Math/MathML">
            <msub>
              <mover>
                <mi>ùê´</mi>
                <mo>^</mo>
              </mover>
              <mi>ij</mi>
            </msub>
          </math>
          . Because these vectors depend only on the relative positions of
          atoms, they rotate consistently with any transformation from SO(3).
        </p>

        <p>
          Strawberry and Vanilla, however, use a slightly different layer architecture. As
          input, instead of zero initialization, as is used in Chocolate,
          information about the molecular geometry is given through using atomic
          position vectors as the vector features of the first layer:
        </p>
        <br />
        <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
          <mrow>
            <msubsup>
              <mi>ùêØ</mi>
              <mi>i</mi>
              <mrow>
                <mo>(</mo>
                <mn>0</mn>
                <mo>)</mo>
              </mrow>
            </msubsup>
            <mo>=</mo>
            <msub>
              <mi>ùê´</mi>
              <mi>i</mi>
            </msub>
            <mo>‚äó</mo>
            <msub>
              <mn>ùüè</mn>
              <mi>F</mi>
            </msub>
          </mrow>
        </math>
        <br />

        <p>Where, as before, <math><mi>F</mi></math> is the hidden dimension. A single layer of Strawberry:</p>

        <ol>
          <li>
            Computes a "pseudo-position" and "pseudo-distance" from the current
            vector features:
          </li>

          <br />

          <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
            <mrow>
              <msubsup>
                <mover>
                  <mi>ùê´</mi>
                  <mo>~</mo>
                </mover>
                <mi>i</mi>
                <mrow>
                  <mo>(</mo>
                  <mi>l</mi>
                  <mo>)</mo>
                </mrow>
              </msubsup>
              <mo>=</mo>
              <mfrac>
                <mn>1</mn>
                <mi>F</mi>
              </mfrac>
              <munderover>
                <mo movablelimits="false">‚àë</mo>
                <mrow>
                  <mi>k</mi>
                  <mo>=</mo>
                  <mn>1</mn>
                </mrow>
                <mi>F</mi>
              </munderover>
              <msubsup>
                <mi>ùêØ</mi>
                <mrow>
                  <mi>i</mi>
                  <mo>,</mo>
                  <mo>:</mo>
                  <mo>,</mo>
                  <mi>k</mi>
                </mrow>
                <mrow>
                  <mo>(</mo>
                  <mi>l</mi>
                  <mo>)</mo>
                </mrow>
              </msubsup>
            </mrow>
          </math>

          <br />

          <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
            <mrow>
              <msubsup>
                <mover>
                  <mi>ùê´</mi>
                  <mo>~</mo>
                </mover>
                <mi>ij</mi>
                <mrow>
                  <mo>(</mo>
                  <mi>l</mi>
                  <mo>)</mo>
                </mrow>
              </msubsup>
              <mo>=</mo>
              <msubsup>
                <mover>
                  <mi>ùê´</mi>
                  <mo>~</mo>
                </mover>
                <mi>j</mi>
                <mrow>
                  <mo>(</mo>
                  <mi>l</mi>
                  <mo>)</mo>
                </mrow>
              </msubsup>
              <mo>‚àí</mo>
              <msubsup>
                <mover>
                  <mi>ùê´</mi>
                  <mo>~</mo>
                </mover>
                <mi>i</mi>
                <mrow>
                  <mo>(</mo>
                  <mi>l</mi>
                  <mo>)</mo>
                </mrow>
              </msubsup>
            </mrow>
          </math>

          <br />

          <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
            <mrow>
              <msubsup>
                <mover>
                  <mi>d</mi>
                  <mo>~</mo>
                </mover>
                <mi>ij</mi>
                <mrow>
                  <mo>(</mo>
                  <mi>l</mi>
                  <mo>)</mo>
                </mrow>
              </msubsup>
              <mo>=</mo>
              <msup>
                <mrow>
                  <mo>‚Äñ</mo>
                  <msubsup>
                    <mover>
                      <mi>ùê´</mi>
                      <mo>~</mo>
                    </mover>
                    <mi>ij</mi>
                    <mrow>
                      <mo>(</mo>
                      <mi>l</mi>
                      <mo>)</mo>
                    </mrow>
                  </msubsup>
                  <mo>‚Äñ</mo>
                </mrow>
                <mn>2</mn>
              </msup>
            </mrow>
          </math>

          <br />

          <li>
            Creates a flattened representation
            <math xmlns="http://www.w3.org/1998/Math/MathML">
              <msubsup>
                <mover>
                  <mi>ùêØ</mi>
                  <mo>~</mo>
                </mover>
                <mi>i</mi>
                <mrow>
                  <mo>(</mo>
                  <mi>l</mi>
                  <mo>)</mo>
                </mrow>
              </msubsup>
            </math>
            of the vector features and concatenates it with the scalar features:
          </li>

          <br />

          <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
            <mrow>
              <msubsup>
                <mover>
                  <mi>ùêØ</mi>
                  <mo>~</mo>
                </mover>
                <mi>i</mi>
                <mrow>
                  <mo>(</mo>
                  <mi>l</mi>
                  <mo>)</mo>
                </mrow>
              </msubsup>
              <mo>=</mo>
              <mi>flatten</mi>
              <mo>(</mo>
              <msubsup>
                <mi>ùêØ</mi>
                <mi>i</mi>
                <mrow>
                  <mo>(</mo>
                  <mi>l</mi>
                  <mo>)</mo>
                </mrow>
              </msubsup>
              <mo>)</mo>
            </mrow>
          </math>

          <br />

          <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
            <mrow>
              <msubsup>
                <mi>u</mi>
                <mi>i</mi>
                <mrow>
                  <mo>(</mo>
                  <mi>l</mi>
                  <mo>)</mo>
                </mrow>
              </msubsup>
              <mo>=</mo>
              <mo>(</mo>
              <msubsup>
                <mi>x</mi>
                <mi>i</mi>
                <mrow>
                  <mo>(</mo>
                  <mi>l</mi>
                  <mo>)</mo>
                </mrow>
              </msubsup>
              <mo>,</mo>
              <msubsup>
                <mover>
                  <mi>ùêØ</mi>
                  <mo>~</mo>
                </mover>
                <mi>i</mi>
                <mrow>
                  <mo>(</mo>
                  <mi>l</mi>
                  <mo>)</mo>
                </mrow>
              </msubsup>
              <mo>)</mo>
            </mrow>
          </math>

          <br />

          <li>Computes messages with an MLP:</li>

          <br />

          <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
            <mrow>
              <msub>
                <mi>m</mi>
                <mi>ij</mi>
              </msub>
              <mo>=</mo>
              <msub>
                <mi>MLP</mi>
                <mtext>msg</mtext>
              </msub>
              <mo>(</mo>
              <msubsup>
                <mi>u</mi>
                <mi>i</mi>
                <mrow>
                  <mo>(</mo>
                  <mi>l</mi>
                  <mo>)</mo>
                </mrow>
              </msubsup>
              <mo>,</mo>
              <msubsup>
                <mi>u</mi>
                <mi>j</mi>
                <mrow>
                  <mo>(</mo>
                  <mi>l</mi>
                  <mo>)</mo>
                </mrow>
              </msubsup>
              <mo>,</mo>
              <msubsup>
                <mover>
                  <mi>d</mi>
                  <mo>~</mo>
                </mover>
                <mi>ij</mi>
                <mrow>
                  <mo>(</mo>
                  <mi>l</mi>
                  <mo>)</mo>
                </mrow>
              </msubsup>
              <mo>)</mo>
            </mrow>
          </math>

          <br />

          <li>
            Splits the output of the message MLP into vector gates and scalar
            messages:
          </li>

          <br />

          <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
            <mrow>
              <mo>(</mo>
              <msubsup>
                <mi>m</mi>
                <mi>ij</mi>
                <mtext>scalar</mtext>
              </msubsup>
              <mo>,</mo>
              <msubsup>
                <mi>m</mi>
                <mi>ij</mi>
                <mtext>flat</mtext>
              </msubsup>
              <mo>)</mo>
              <mo>=</mo>
              <mi>split</mi>
              <mo>(</mo>
              <msub>
                <mi>m</mi>
                <mi>ij</mi>
              </msub>
              <mo>)</mo>
            </mrow>
          </math>

          <br />

          <li>Reshapes the vector message:</li>

          <br />

          <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
            <mrow>
              <msubsup>
                <mi>m</mi>
                <mi>ij</mi>
                <mtext>vec</mtext>
              </msubsup>
              <mo>=</mo>
              <mi>reshape</mi>
              <mo>(</mo>
              <msubsup>
                <mi>m</mi>
                <mi>ij</mi>
                <mtext>flat</mtext>
              </msubsup>
              <mo>)</mo>
            </mrow>
          </math>

          <br />

          <li>Aggregates the messages:</li>

          <br />

          <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
            <mrow>
              <msub>
                <mi>agg</mi>
                <mi>x</mi>
              </msub>
              <mo stretchy="false">(</mo>
              <mi>i</mi>
              <mo stretchy="false">)</mo>
              <mo>=</mo>
              <munder>
                <mo movablelimits="false">‚àë</mo>
                <mrow>
                  <mi>j</mi>
                  <mo>‚â†</mo>
                  <mi>i</mi>
                </mrow>
              </munder>
              <msubsup>
                <mi>m</mi>
                <mi>ij</mi>
                <mtext>scalar</mtext>
              </msubsup>
            </mrow>
          </math>

          <br />

          <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
            <mrow>
              <msub>
                <mi>agg</mi>
                <mi>ùêØ</mi>
              </msub>
              <mo stretchy="false">(</mo>
              <mi>i</mi>
              <mo stretchy="false">)</mo>
              <mo>=</mo>
              <munder>
                <mo movablelimits="false">‚àë</mo>
                <mrow>
                  <mi>j</mi>
                  <mo>‚â†</mo>
                  <mi>i</mi>
                </mrow>
              </munder>
              <msubsup>
                <mi>m</mi>
                <mi>ij</mi>
                <mtext>vec</mtext>
              </msubsup>
            </mrow>
          </math>

          <br />

          <li>
            Mixes the hidden channels of the vector features (applies a single
            linear layer):
          </li>
          <br />

              <div class="equation-container">
              <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
                <mrow>
                  <msubsup>
                    <mi>ùêØ</mi>
                    <mi>i</mi>
                    <mtext>mix</mtext>
                  </msubsup>
                  <mo>=</mo>
                  <msubsup>
                    <mi>ùêØ</mi>
                    <mi>i</mi>
                    <mrow>
                      <mo>(</mo>
                      <mi>l</mi>
                      <mo>)</mo>
                    </mrow>
                  </msubsup>
                  <msub>
                    <mi>W</mi>
                    <mtext>mix</mtext>
                  </msub>
                </mrow>
              </math>
            </div>


          <br />

          <li>And finally applies the updates:</li>

          <br />

          <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
            <mrow>
              <msubsup>
                <mi>x</mi>
                <mi>i</mi>
                <mrow>
                  <mo>(</mo>
                  <mi>l</mi>
                  <mo>+</mo>
                  <mn>1</mn>
                  <mo>)</mo>
                </mrow>
              </msubsup>
              <mo>=</mo>
              <msubsup>
                <mi>x</mi>
                <mi>i</mi>
                <mrow>
                  <mo>(</mo>
                  <mi>l</mi>
                  <mo>)</mo>
                </mrow>
              </msubsup>
              <mo>+</mo>
              <msub>
                <mi>MLP</mi>
                <mtext>upd</mtext>
              </msub>
              <mo>(</mo>
              <msubsup>
                <mi>x</mi>
                <mi>i</mi>
                <mrow>
                  <mo>(</mo>
                  <mi>l</mi>
                  <mo>)</mo>
                </mrow>
              </msubsup>
              <mo>,</mo>
              <msub>
                <mi>agg</mi>
                <mi>x</mi>
              </msub>
              <mo>(</mo>
              <mi>i</mi>
              <mo>)</mo>
              <mo>)</mo>
            </mrow>
          </math>

          <br />

          <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
            <mrow>
              <msubsup>
                <mi>ùêØ</mi>
                <mi>i</mi>
                <mrow>
                  <mo>(</mo>
                  <mi>l</mi>
                  <mo>+</mo>
                  <mn>1</mn>
                  <mo>)</mo>
                </mrow>
              </msubsup>
              <mo>=</mo>
              <msubsup>
                <mi>ùêØ</mi>
                <mi>i</mi>
                <mrow>
                  <mo>(</mo>
                  <mi>l</mi>
                  <mo>)</mo>
                </mrow>
              </msubsup>
              <mo>+</mo>
              <msub>
                <mi>agg</mi>
                <mi>ùêØ</mi>
              </msub>
              <mo>(</mo>
              <mi>i</mi>
              <mo>)</mo>
              <mo>+</mo>
              <msubsup>
                <mi>ùêØ</mi>
                <mi>i</mi>
                <mtext>mix</mtext>
              </msubsup>
            </mrow>
          </math>
        </ol>

        <h3>Directly penalizing deviations from equivariance</h3>

        <p>
          To encourage Strawberry to be equivariant, we had the model output a
          list of the intermediate vector features as well as the final vector
          dipole prediction. During training with data augmentation, we examined
          the effect of adding an extra loss term that penalized non-equivariant
          representations of intermediate vector features in Strawberry:
        </p>

        <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mrow>
    <mi>L</mi><msub><mi></mi><mrow><mi>eq</mi></mrow></msub>
    <mo>=</mo>
    <mfrac><mn>1</mn><mi>N</mi></mfrac>
    <munderover>
      <mo>&#8721;</mo>
      <mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow>
      <mi>N</mi>
    </munderover>
    <munderover>
      <mo>&#8721;</mo>
      <mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow>
      <mi>L</mi>
    </munderover>
    <msup>
      <mfenced>
        <mrow>
          <msub><mi>h</mi><mi>l</mi></msub>
          <mo>(</mo><mi>R</mi><msub><mi>x</mi><mi>n</mi></msub><mo>)</mo>
          <mo>&#x2212;</mo>
          <mi>R</mi>
          <msub><mi>h</mi><mi>l</mi></msub>
          <mo>(</mo><msub><mi>x</mi><mi>n</mi></msub><mo>)</mo>
        </mrow>
      </mfenced>
      <mn>2</mn>
    </msup>
  </mrow>
</math>

        <p>
          Adding this loss term is tantamount to adding an extra objective. To
          avoid the model getting stuck in a local minimum by predicting a
          trivial dipole (the zero vector) that perfectly satisfies
          equivariance, we scale this additional loss term by a hyperparameter
          Œª. For future implementations of such a layerwise loss, we will
          perform a hyperparameter search to find optimal values of Œª.
          Non-equivariant models with sufficient expressivity and information
          about geometry are known to be capable of learning equivariance. This
          both follows from universal approximation and has been
          shown in applications. [<a href="#ref11">11</a>, <a href="#ref11">12</a>]. To evaluate
          whether Strawberry was able to converge to equivariance with normal,
          data-augmented, and our layerwise loss regimes, we calculated the
          equivariance error of the predicted dipoles and the latent vector
          representations over a random sample of molecules with 4 rotations
          applied to them.
        </p>
      </div>
      <div class="margin-right-block">
        ‚äó symbol stands for Tensor product.
      </div>
    </div>

		<div class="content-margin-container" id="results">
			<div class="margin-left-block"></div>
		  <div class="main-content-block">
				<h1>Results</h1>
				
				<p>
					Our goal was to evaluate whether Strawberry, a non-equivariant GNN, could get approximate 
          rotational equivariance through two mechanisms: rotational data augmentation and per-layer 
          equivariance penalties. We compared these variants against Chocolate, a model with 
          built-in equivariant architecture. The combined results suggest that while augmentation 
          encourages some degree of equivariance, na√Øvely applying an MSE-based equivariance regularizer 
          introduces optimization instabilities that degrade predictive accuracy. We evaluated all models on 
          dipole prediction for the QM7-X test set:
				</p>
				<h3>RMSE Accuracy on Test Set</h3>
        <table class="booktabs">
          <caption>Dipole prediction on QM7-X (test set)</caption>
          <thead>
            <tr>
              <th>Model</th>
              <th class="num">MSE</th>
              <th class="num">RMSE</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Chocolate</td>
              <td class="num">0.0044</td>
              <td class="num">0.0670</td>
            </tr>
            <tr>
              <td>Vanilla</td>
              <td class="num">0.0071</td>
              <td class="num">0.0840</td>
            </tr>
            <tr>
              <td>Strawberry with Augmentation</td>
              <td class="num">0.0090</td>
              <td class="num">0.0950</td>
            </tr>
            <tr>
              <td>Strawberry with Augmentation and Layerwise Losses</td>

              <td class="num">0.0097</td>
              <td class="num">0.0982</td>
            </tr>
          </tbody>
        </table>
				<p>
					While none of the models reached state-of-the-art performance, all models achieved some predictive accuracy.
           As expected, Chocolate achieved the best RMSE, however, Vanilla performed surprisingly well relative to Strawberry with augmentation. 
           We were surprised to see that the per-layer equivariance loss further degraded RMSE, 
           suggesting that simple MSE regularizer may have encouraged latent collapse instead of learning equivariance.
					</p>
	    	</div>
				<div class="margin-right-block"></div>
			</div>

			<div class="content-margin-container">
				<div class="margin-left-block"></div>
				<div class="main-content-block">
					<h3>Training Loss Curves</h3>
					<p>
  Training and validation trajectories reveal how each training schedule affects optimization:
</p>

<ul>
  <li>
    <strong>Chocolate</strong> exhibits smooth convergence and low validation variance, consistent with the stability typically observed in equivariant architectures.
  </li>
  <li>
    <strong>Strawberry (no augmentation)</strong> trains cleanly but underfits compared to Chocolate.
  </li>
  <li>
    <strong>Strawberry + augmentation</strong> shows noisier validation curves, reflecting the harder learning task introduced by rotational variation.
  </li>
  <li>
    <strong>Strawberry + augmentation + layerwise loss</strong> converges the slowest and plateaus at the highest loss. Even with learning-rate scheduling, the validation curves are notably chaotic.
  </li>
</ul>

<p>
  These patterns suggest that the intermediate per-layer losses significantly complicate optimization. A lower learning rate and longer training horizon, combined with smaller batches, may help stabilize training in future work.
</p>

					</p>
					<img src="./images/loss_curves_grid.svg" width="512" />
				</div>
				<div class="margin-right-block">
					Training/validation loss across augmentation and layerwise-loss schedules
				</div>
			</div>

			<div class="content-margin-container" style="align-items: flex-start;">
				<div class="margin-left-block"></div>
				<div class="main-content-block">
					<h3>Predicted vs. True Dipoles</h3>
					<p>
						To visualize predictive performance, we plotted predicted versus ground-truth dipole components for all model variants (Fig. 4). The Chocolate model produces the tightest clustering around the identity line, consistent with its lowest RMSE and built-in equivariance. Both Vanilla and Strawberry + augmentation show broader scatter, indicating less stable vector predictions. Adding layerwise equivariance losses worsens the spread further, reflecting the degradation in accuracy observed in the quantitative metrics. Overall, these plots reinforce that the simple equivariance penalty does not translate into improved dipole prediction and can even destabilize the learned vector representations.
					</p>
					<img src="./images/dipole_pred_vs_true_grid.png" width="512" />
				</div>
				<div class="margin-right-block">
					2√ó2 grid: Vanilla, Strawberry+Aug, Strawberry+Aug+Layerwise, Chocolate
				</div>
			</div>
		
			<div class="content-margin-container">
				<div class="margin-left-block"></div>
				<div class="main-content-block">
	        <h3>Layerwise Equivariance Analysis</h3>
	        <p>To quantify how well each model learned rotationally stable representations, we computed the 
            equivariance error of each latent vector over 64 SO(3) rotations sampled using a Superfibonacci 
            grid and plotted distributions as violin plots.
        </p>
					<img src="./images/equivariance_violin_grid.svg" width="512" />
        <p>
          As seen from violin grins, Chocolate demonstrates near-perfect equivariance at all layers. Vanilla and Strawberry with augmentation 
          show similar distributions with substantial equivariance error, though augmentation slightly improves stability in early layers. 
          Unexpectedly, strawberry + augmentation + layerwise losses show dramatic improvement at layer 0 and 1, but layer 2 exhibits persistent high error, 
          contrary to the desired effect of the regularizer. This failure mode is strongly suggestive of representational collapse in the earlier layers, 
          which prompted us to examine mean vector norms.
        </p>
					<img src="./images/equivariance_norms_grid.svg" width="512" />
        <p style="font-size: 14px; color: #555; text-align: center; margin-top: 4px;">
          Vector Norm Analysis
        </p>
        <p>
          As expected, Chocolate, which has an equivariant structure, has equivariance errors on the order
          of floating point noise.  Vanilla and Strawberry with augmentation have comparable equivariance losses, with
          Strawberry having slightly lower errors.  Unexpectedly, the equivariance regularizer seemed to work on the
          first two layers Strawberry when added, but it mostly failed on the last layer.  We plotted the vector norms,
          and the clear difference between the mean vector norms for the first two and last layer indicate that the
          layerwise loss encouraged Strawberry to collapse representations.
          For future implementations, we believe that normalizing the simple MSE objective may help to
          solve this collapsing norm problem.
        </p>
				</div>
	     			<div class="margin-right-block">
					Violin plots for per-layer equivariance and per-layer mean vector norms.
				</div>
			</div>
  

    <div class="content-margin-container" id="conclusion">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Conclusion</h1>
        <p>
          Our results show that while rotational augmentation provides a weak equivariance prior, it does not approach the stability or accuracy of an equivariant architecture like Chocolate. More notably, introducing per-layer equivariance penalties in Strawberry leads to a failure mode: representations in early layers collapse, yielding low equivariance error only because the latent vectors shrink toward zero. This phenomenon appeared consistently across seeds and training attempts, indicating that it is not an artifact of specific hyperparameters but a structural issue with the regularizer.
</p>
<p>
Because our experiments were constrained by compute and time, we did not perform systematic sweeps over Œª, augmentation multiplicity, hidden size, or model depth. Therefore, our study should be viewed as evaluating a single point in a larger design space. Nonetheless, the emergence of latent collapse is itself an informative result‚Äîit implies that na√Øve equivariance penalties may be fundamentally incompatible with non-equivariant message-passing networks unless carefully normalized or re-designed.
        </p>
      </div>
      <div class="margin-right-block"></div>
    </div>

    <div class="content-margin-container" id="implications_and_limitations">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Implications and limitations</h1>

        <p>
          Training models to approximately understand physics constraints through data augmentation and equivariance
          regularization is an active area of research.  Our work demonstrates that geometric regularization of a 
          model's latent representation can be implemented with very simple terms, customizable to any symmetry of
          interest.  We also demonstrated, however, that these regularizations can be harmful to a model's quality when
          a push towards a desired physical constraint is in local tension with the prediction task.  While not needing the
          same amount of hard-coded structure as a hard imposition of equivariance upon a model, this tension must be
          recognized and mitigated.
        </p>
      </div>
      <div class="margin-right-block"></div>
    </div>

    <div class="content-margin-container" id="citations">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <div class="citation" id="references" style="height: auto">
          <br />
          <h2>References:</h2><br /><br />
<ol>
  <li id="ref1"> 
    <a href="https://arxiv.org/abs/1706.08566" target="_blank">Sch√ºtt, K. T., et al. (2017).</a> SchNet: A continuous-filter convolutional neural network for modeling quantum interactions.
    <i>NeurIPS.</i>
  </li>

  <li id="ref2">
    <a href="https://arxiv.org/abs/2102.03150" target="_blank">Sch√ºtt, K. T., et al. (2021).</a> Equivariant message passing for the prediction of tensorial properties (PaiNN).
    <i>ICML.</i>
  </li>

  <li id="ref3">
    <a href="https://arxiv.org/abs/2102.09844" target="_blank">Satorras, V. G., et al. (2021).</a> E(n) Equivariant Graph Neural Networks.
    <i>ICML.</i> 
  </li>

  <li id="ref4">
    <a href="https://arxiv.org/abs/1802.08219" target="_blank">Thomas, N., et al. (2018).</a> Tensor Field Networks: Rotation- and translation-equivariant neural networks for 3D point clouds.
    <i>ICLR.</i>
  </li>

  <li id="ref5">
    <a href="https://arxiv.org/abs/2006.10503" target="_blank">Fuchs, F. B., et al. (2020).</a> SE(3)-Transformers: 3D equivariant attention networks.
    <i>NeurIPS.</i>
  </li>

  <li id="ref6"> 
    <a href="https://www.nature.com/articles/s41597-021-00882-8" target="_blank">Hoja, J., et al. (2021).</a> QM7-X: A comprehensive dataset of quantum-mechanical properties.
    <i>Scientific Data.</i>
  </li>

    <li id="ref7">
    <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Alexa_Super-Fibonacci_Spirals_Fast_Low-Discrepancy_Sampling_of_SO3_CVPR_2022_paper.pdf" target="_blank">Alexa, M. (2022).</a> Super-Fibonacci Spirals: Fast, Low-Discrepancy Sampling of SO(3).
    <i>CVPR 2022.</i>
  </li>

  <li id="ref8">
    <a href="https://pubs.acs.org/doi/10.1021/ci00057a005" target="_blank">Weininger, D. (1988).</a> SMILES, a chemical language and information system. <i>J. Chem. Inf. Comput. Sci. </i>
  </li>

  <li id="ref9">
    <a href="https://pubs.acs.org/doi/10.1021/ci100050t" target="_blank">Rogers, D., Hahn, M. (2010).</a> Extended-connectivity fingerprints. <i>J. Chem. Inf. Model. </i>
  </li>

  <li id="ref10">
    <a href="https://arxiv.org/pdf/2511.07087" target="_blank">Filling, J. P. (2025).</a> Direct Molecular Polarizability Prediction with
SO(3) Equivariant Local Frame GNNs. <i>arXiv preprint.</i>
  </li>

  <li id="ref11">
    <a href="https://www.sciencedirect.com/science/article/pii/S0893608005801315" target="_blank">Leshno, M. et al. (1993).</a> Multilayer feedforward networks with a nonpolynomial activation function can approximate any function. 
    <i>Neural Networks.</i>
  </li>
  <li id="ref12">
    <a href="https://arxiv.org/abs/2006.16221" target="_blank">Finzi M., et al. (2021).</a> Approximate Equivariance Learning in Neural Networks.
    <i>Neural Networks.</i>
  </li>
</ol>

        </div>
      </div>
      <div class="margin-right-block"></div>
    </div>
	<script>
  $(function () {
    $('math[display="block"]').each(function () {
      if (!$(this).parent().hasClass('equation-container')) {
        $(this).wrap('<div class="equation-container"></div>');
      }
    });
  });
</script>

  </body>
</html>
