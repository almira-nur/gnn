<html>

<head>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>

<link rel="shortcut icon" href="images/icon.ico">

<style type="text/css">

	body {
		background-color: #f5f9ff;
	}

	math {
		font-family: "STIX Two Math","Cambria Math","Latin Modern Math",serif;
		line-height: 1.2;
	}

	math[display="block"] {
		display: block;
		margin: 0 auto 8px;
		text-align: center;
	}
	/* Hide both math displays initially, will display based on JS detection */
  .mathjax-mobile, .mathml-non-mobile { display: none; }
  /* Show the MathML content by default on non-mobile devices */
  .show-mathml .mathml-non-mobile { display: block; }
  .show-mathjax .mathjax-mobile { display: block; }

	.content-margin-container {
		display: flex;
		width: 100%; /* Ensure the container is full width */
		justify-content: left; /* Horizontally centers the children in the container */
		align-items: center;  /* Vertically centers the children in the container */
	}

	.main-content-block {
		width: 70%; /* Change this percentage as needed */
    max-width: 1100px; /* Optional: Maximum width */
		background-color: #fff;
		border-left: 1px solid #DDD;
		border-right: 1px solid #DDD;
		padding: 8px 8px 8px 8px;
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
	}

	.margin-left-block {
			font-size: 14px;
			width: 15%; /* Change this percentage as needed */
			max-width: 130px; /* Optional: Maximum width */
			position: relative;
			margin-left: 10px;
			text-align: left;
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
			padding: 5px;
	}

	.margin-right-block {
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
			font-size: 14px;
			width: 25%; /* Change this percentage as needed */
			max-width: 256px; /* Optional: Maximum width */
			position: relative;
			text-align: left;
			padding: 10px;  /* Optional: Adds padding inside the caption */
	}

	img {
			max-width: 100%; /* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
	}

	.my-video {
			max-width: 100%; /* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
	}

	/* Hide both video displays initially, will display based on JS detection */
  .vid-mobile, .vid-non-mobile { display: none; }
  /* Show the video content by default on non-mobile devices */
  .show-vid-mobile .vid-mobile { display: block; }
  .show-vid-non-mobile .vid-non-mobile { display: block; }

	a:link,a:visited
	{
		color: #0e7862; /*#1367a7;*/
		text-decoration: none;
	}

	a:hover {
		color: #24b597; /*#208799;*/
	}

	h1 {
		font-size: 18px;
		margin-top: 4px;
		margin-bottom: 10px;
	}

	h2 {
		font-size: 20px;
		margin-top: 12px;
		margin-bottom: 8px;
	}

	h3 {
		font-size: 16px;
		margin-top: 10px;
		margin-bottom: 6px;
	}

	table.header {
    font-weight: 300;
    font-size: 17px;
    flex-grow: 1;
		width: 70%;
    max-width: calc(100% - 290px); /* Adjust according to the width of .paper-code-tab */
	}

	table td, table td * {
	    vertical-align: middle;
	    position: relative;
	}

	table.paper-code-tab {
	    flex-shrink: 0;
	    margin-left: 8px;
	    margin-top: 8px;
	    padding: 0px 0px 0px 8px;
	    width: 290px;
	    height: 150px;
	}

	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	hr {
    height: 1px; /* Sets the height of the line to 1 pixel */
    border: none; /* Removes the default border */
    background-color: #DDD; /* Sets the line color to black */
  }

	div.hypothesis {
		width: 80%;
		background-color: #EEE;
		border: 1px solid black;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
		font-family: Courier;
		font-size: 18px;
		text-align: center;
		margin: auto;
		padding: 16px 16px 16px 16px;
	}

	div.citation {
    font-size: 0.8em;
    background-color:#fff;
    padding: 10px;
		height: 200px;
  }

	.fade-in-inline {
		position: absolute;
		text-align: center;
		margin: auto;
		-webkit-mask-image: linear-gradient(to right,
																			transparent 0%,
																			transparent 40%,
																			black 50%,
																			black 90%,
																			transparent 100%);
		mask-image: linear-gradient(to right,
																transparent 0%,
																transparent 40%,
																black 50%,
																black 90%,
																transparent 100%);
		-webkit-mask-size: 8000% 100%;
		mask-size: 8000% 100%;
		animation-name: sweepMask;
		animation-duration: 4s;
		animation-iteration-count: infinite;
		animation-timing-function: linear;
		animation-delay: -1s;
	}

	.fade-in2-inline {
			animation-delay: 1s;
	}

	.inline-div {
			position: relative;
	    display: inline-block; /* Makes both the div and paragraph inline-block elements */
	    vertical-align: top; /* Aligns them at the top, you can adjust this to middle, bottom, etc., based on your needs */
	    width: 50px; /* Optional: Adds space between the div and the paragraph */
	}

	/* Simple layout for model diagrams */
	.model-diagrams {
		display: flex;
		flex-wrap: wrap;
		gap: 16px;
		margin: 16px 0;
	}

	.model-diagrams figure {
		flex: 1 1 200px;
		text-align: center;
		font-size: 14px;
	}

	.model-diagrams figcaption {
		margin-top: 6px;
		color: #555;
	}

</style>

	  <title>The Platonic Representation Hypothesis</title>

      <meta property="og:title" content="The Platonic Representation Hypothesis" />
			<meta charset="UTF-8">
  </head>

  <body>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<table class="header" align=left>
								<tr>
									<td colspan=4>
										<span style="font-size: 32px; font-family: 'Courier New', Courier, monospace; /* Adds fallbacks */">A deeper look into equivariance for molecular data</span>
									</td>
								</tr>
								<tr>
										<td align=left>
												<span style="font-size:17px"><a href="your_website">Timothy Pinkhassik</a></span>
										</td>
										<td align=left>
												<span style="font-size:17px"><a href="your_partner's_website">Almira Nurlanova</a></span>
										</td>
								<tr>
									<td colspan=4 align=left><span style="font-size:18px">Final project for 6.7960, MIT</span></td>
								</tr>
						</table>
					</div>
					<div class="margin-right-block">
					</div>
		</div>

		<div class="content-margin-container" id="intro">
				<div class="margin-left-block">
          <!-- table of contents here -->
          <div style="position:fixed; max-width:inherit; top:max(20%,120px)">
              <b style="font-size:16px">Outline</b><br><br>
              <a href="#intro">Introduction</a><br><br>
              <a href="#background">Background</a><br><br>
							<a href="#related_works">Related Works</a><br><br>
			  			<a href="#methods">Methods</a><br><br>
							<a href="#results">Results</a><br><br>
			  			<a href="#evaluation">Evaluation</a><br><br>
			  			<a href="#conclusion">Conclusion</a><br><br>
              <a href="#implications_and_limitations">Implications and limitations</a><br><br>
          </div>
				</div>
		    <div class="main-content-block">
            <!--You can embed an image like this:-->
            <img src="./images/your_image_here.png" width=512px/>
		    </div>
		    <div class="margin-right-block">
						Caption for the image.
		    </div>
		</div>

    <div class="content-margin-container" id="intro-main">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h2>Introduction</h2>

						<p>
							A Graph Neural Network (GNN) is one of the most naturally suitable networks to represent molecular data, where a node corresponds to an atom and an edge corresponds to a chemical bond. Because of this structural alignment, there has been immense research done on using GNNs to work with molecular data in various applications like property prediction, organic retrosynthesis, and protein‚Äìligand binding. However, by default GNNs do not inherently encode one of the central properties of molecules ‚Äì rotational and translational (SE(3)) equivariance.
						</p>

						<p>
							To address this limitation, a number of architectures with built-in equivariance have been proposed such as PaiNN, SchNet, and EGNN. Despite achieving better performance and more meaningful and accurate physical representation, built-in equivariant GNNs are computationally expensive, often relying on spherical harmonics or per-edge vector updates.
						</p>

						<p>
							This raises an important question of effectiveness and design trade-off: could a carefully designed non-equivariant GNN, trained on synthetically augmented data, learn equivariance and achieve similar results while requiring fewer computations? To investigate this hypothesis, we designed a set of models to predict the dipole moment of small organic molecules from the QM7-X dataset, avoiding models that rely on spherical harmonics due to their higher computational complexity.
						</p>

						<p>
							Dipole moment prediction is a particularly suitable benchmark for this case as it is a 3D-vector quantity; as a molecule gets rotated, the output should rotate in the same direction while retaining magnitude.
						</p>

						<p>
	Our central hypothesis is that a non-equivariant GNN trained with sufficiently diverse rotational augmentation can approximate SE(3) equivariance closely enough to perform competitively with an explicitly equivariant architecture, at reduced computational cost.
</p>

						<br/>

						<p>To evaluate the effectiveness of equivariance learning we compared these three cases:</p>

						<ol>
							<li>
								<b>Vanilla</b> ‚Äì a standard, non-equivariant GNN that was trained on the original dataset without rotational augmentation.
							</li>
							<br/>
							<li>
								<b>Strawberry</b> ‚Äì a standard, non-equivariant GNN, which has the same architecture as the Vanilla model, trained on data augmentation. We used superfibonacci sampling to obtain rotated data from our training dataset.
							</li>
							<br/>
							<li>
								<b>Chocolate</b> ‚Äì our built-in equivariant GNN that was designed such that internal vector features are guaranteed to rotate with rotation of the input. It does not rely on spherical harmonics, which would substantially increase computational cost, but instead adds vector features to each node, similar in spirit to PaiNN and EGNN.
							</li>
						</ol>

						<div class="model-diagrams">
							<figure>
								<img src="./images/model_vanilla.png" alt="Vanilla GNN diagram">
								<figcaption>Vanilla: non-equivariant GNN trained on original data.</figcaption>
							</figure>
							<figure>
								<img src="./images/model_strawberry.png" alt="Strawberry GNN diagram">
								<figcaption>Strawberry: same architecture with rotationally augmented training data.</figcaption>
							</figure>
							<figure>
								<img src="./images/model_chocolate.png" alt="Chocolate GNN diagram">
								<figcaption>Chocolate: built-in equivariant GNN with vector features.</figcaption>
							</figure>
						</div>

						<hr/>

						<!-- BACKGROUND + FORMAL DEFINITIONS INSIDE SAME BLOCK -->
						<a id="background"></a>
						<h2>Background</h2>

						<h3>Latent Space</h3>
						<p>
							Deep learning models for molecules operate by mapping molecular structures into a latent space, where each node and edge is represented by learned features. For a GNN, we typically distinguish between scalar node features
							<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>h</mi><mo>‚àà</mo><msup><mi>‚Ñù</mi><mi>d</mi></msup></math>
							and vector features
							<math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="bold">v</mi><mo>‚àà</mo><msup><mi>‚Ñù</mi><mn>3</mn></msup></math>.
							Scalar features encode information like atom type or charge, while vector features encode directional or geometric information tied to 3D space.
						</p>
						<p>
							In non-equivariant GNNs, the geometry of this latent space does not necessarily transform consistently under rotations of the input. In contrast, equivariant GNNs constrain vector features so that when the input coordinates are rotated by a rotation matrix
							<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>R</mi><mo>‚àà</mo><mi>SO</mi><mo>(</mo><mn>3</mn><mo>)</mo></math>,
							the corresponding vector features transform as
							<math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="bold">v</mi><mo>‚Ü¶</mo><mi>R</mi><mi mathvariant="bold">v</mi></math>.
							This structured latent space is crucial for correctly modeling vector-valued properties such as dipole moments.
						</p>

						<h3>Equivariance</h3>

						<p>
							We formulated two simple GNN models ‚Äî Strawberry and Chocolate ‚Äî to examine how models could learn equivariance. We treat a molecule as a tuple
							<math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="bold">M</mi></math>:
						</p>

						<!-- Molecule definition -->
						<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
							<mi mathvariant="bold">M</mi>
							<mo>=</mo>
							<mrow>
								<mo>(</mo>
								<mi>V</mi><mo>,</mo>
								<mi>E</mi><mo>,</mo>
								<mi>z</mi><mo>,</mo>
								<mi mathvariant="bold">r</mi><mo>,</mo>
								<mi mathvariant="bold">Œº</mi>
								<mo>)</mo>
							</mrow>
						</math>

						<p>
							Here, <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>V</mi></math> is the set of nodes (atoms),
							<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>E</mi></math> is the set of edges (we treat the molecule as a point cloud so there are
							<math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>N</mi><mn>2</mn></msup><mo>‚àí</mo><mi>N</mi></math> directed edges),
							<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>z</mi><mi>i</mi></msub></math> are node labels (atomic identities),
							<math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="bold">r</mi><mo>=</mo><mrow><mo>{</mo><msub><mi>r</mi><mi>i</mi></msub><mo>}</mo></mrow></math> are 3D coordinates of atoms, and
							<math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="bold">Œº</mi></math> is the dipole moment vector.
						</p>

						<p>For a rotation <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>R</mi></math> in SO(3), the rotated molecule is:</p>

						<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
							<mi>R</mi><mo>¬∑</mo>
							<mi mathvariant="bold">M</mi>
							<mo>=</mo>
							<mo>(</mo>
							<mi>V</mi><mo>,</mo>
							<mi>E</mi><mo>,</mo>
							<mi>z</mi><mo>,</mo>
							<mi>R</mi><mi mathvariant="bold">r</mi><mo>,</mo>
							<mi>R</mi><mi mathvariant="bold">Œº</mi>
							<mo>)</mo>
						</math>

						<p>
							A model
							<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi></math>
							is equivariant with respect to SO(3) if:
						</p>

						<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
							<mo>‚àÄ</mo><mi>R</mi><mo>‚àà</mo><mi>SO</mi><mo>(</mo><mn>3</mn><mo>)</mo><mo>,</mo>
							<mi>f</mi><mo>(</mo><mi>z</mi><mo>,</mo><mi>R</mi><mi mathvariant="bold">r</mi><mo>,</mo><mi>E</mi><mo>)</mo>
							<mo>=</mo>
							<mi>R</mi>
							<mi>f</mi><mo>(</mo><mi>z</mi><mo>,</mo><mi mathvariant="bold">r</mi><mo>,</mo><mi>E</mi><mo>)</mo>
						</math>

						<p>
							We defined an equivariant GNN, "Chocolate", inspired by the EGNN architecture <a href="#ref_2">[2]</a>, and a non-equivariant GNN, "Strawberry", serving as a control with a similar architecture but lacking equivariant operations. Vanilla serves as a simpler non-equivariant baseline without rotational augmentation.
						</p>

		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>

		<!-- RELATED WORKS SECTION -->
		<div class="content-margin-container" id="related_works">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h2>Related Works</h2>

						<p>
							There have been many attempts to learn the best representation of molecular data for better convergence, including string-based approaches (e.g., SMILES) and fingerprint-based approaches (e.g., ECFP, Morgan Fingerprints). However, these approaches tend to lose structural and geometric information of molecules, especially in 3D, which motivates the use of GNNs over molecular graphs embedded in space.
						</p>

						<h3>Rotational and Translational Invariance</h3>

						<h3>SchNet</h3>
						<p>
							This deep learning architecture uses continuous-filter convolution layers, rather than classic message passing algorithms. Continuous-filter convolution layers make it feasible to work with molecules scattered in 3D space, rather than classic grid-based convolution. The filter is implemented as a neural network; that is, it is a function, not a fixed kernel, that takes a displacement vector and outputs a learned weight. For efficiency, the operation is applied per feature channel.
						</p>
						<p>
							However, SchNet cannot be used to predict vector quantities, as it only guarantees translational and rotational invariance, not equivariance. It outputs scalar invariant features that are well-suited for scalar properties such as energy, but cannot directly represent properties like forces or dipole moments.
						</p>

						<h3>Equivariant Graph Neural Network (EGNN)</h3>
						<p>
							EGNN is a deep learning architecture that solves the SE(3) equivariance problem while additionally introducing reflection equivariance, which makes the architecture E(n)-equivariant. It updates both scalar and vector features and uses relative positions to maintain equivariance. Although the algorithm does not rely on spherical harmonics, it is more expressive than invariant architectures like SchNet, which comes with higher computational cost due to computing vector updates per edge.
						</p>

						<h3>PaiNN</h3>
						<p>
							PaiNN is a tensor-based SE(3)-equivariant network designed as a successor of SchNet. It introduces explicit vector features and uses them to propagate directional information through the model. PaiNN is designed to be more efficient than spherical harmonics-based models while still enforcing equivariance constraints. However, maintaining and updating vector-valued features throughout message passing still incurs additional computational cost compared to purely invariant architectures.
						</p>

						<p>
							Collectively, these works motivate our study: they demonstrate that equivariant architectures can achieve strong performance, but also highlight a computational trade-off. Our work examines whether a non-equivariant GNN with carefully designed data augmentation can recover similar behavior, particularly for vector-valued molecular properties, at reduced computational budget.
						</p>

		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>

	<div class="content-margin-container" id="methods">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>Methods</h1>

			<p>
				For a simple prediction task on molecular data sensitive to 3D geometry, we chose to predict the vector
				molecular dipole moment for small molecules from the QM7x dataset <a href="#ref_2">[2]</a>.
				This dataset contains $\sim$7,000 small molecules with up to 7 "heavy" (non-hydrogen) atoms each, in various
				conformations. We treated each molecule as a point cloud with labeled atom types. The coordinates of
				molecules in QM7x are centered, as are coordinates of other computational datasets. For computational
				tractability, we restricted our analysis to SO(3) equivariance instead of the more popular E(3) objective,
				which avoided the necessity of augmenting data with translations and simplified evaluation. We expect the
				conclusions found from an analysis of SO(3) equivariance to be transferrable to other symmetries.
			</p>

			<h3> Model Architecture</h3>

			<p>
			For each structure with <math><mi>N</mi></math> atoms, we built all
			<math><mrow><msup><mi>N</mi><mn>2</mn></msup><mo>‚àí</mo><mi>N</mi></mrow></math> directed
			edges <math><mrow><mo>(</mo><mi>i</mi><mo>,</mo><mi>j</mi>
			<mo>)</mo></mrow></math> where
			<math><mrow><mi>i</mi><mo>‚â†</mo><mi>j</mi></mrow></math>. Each node was featurized with a learned scalar
			embedding of the atom type from the set
			(H, C, N, O, F, P, S, Cl). Each edge <math><mrow><mo>(</mo><mi>i</mi><mo>,</mo><mi>j</mi>
			<mo>)</mo></mrow></math> was featurized with the distance between atoms <math><mi>i</mi></math> and <math><mi>j</mi></math>.
			We split the dataset into training, validation, and test sets with 80%, 10%, and 10% of molecules,
			respectively, ensuring that different conformations of the same molecule were all in the same split.
			For tractability in training, we randomly downsampled the training set to 200 conformers per each molecule.
			</p>

			<p>
			A single layer of Chocolate maps
			</p>

		<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
<mrow><mo>(</mo><msup><mi>x</mi><mrow><mo>(</mo><mi>l</mi><mo>)</mo></mrow></msup><mo>,</mo><mi mathvariant="bold">v</mi><mo>)</mo><mo>‚Ü¶</mo><mo>(</mo><msup><mi>x</mi><mrow><mo>(</mo><mi>l</mi><mo>+</mo><mn>1</mn><mo>)</mo></mrow></msup><mo>,</mo><mi mathvariant="bold">v</mi><mo>)</mo></mrow>
</math>

<p>
Given a node <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>i</mi></math> with adjacent node <math xmlns="http://www.w3.org/1998/Math/MathML"><mi>j</mi></math>, the layer computes:
</p>

<ol>
<li>The direction of the neighboring node and its distance:</li>
	<br>
	<math display="block"><mrow><msub><mi>ùê´</mi><mi>ij</mi></msub><mo>=</mo><msub><mi>ùê´</mi><mi mathvariant="normal">j</mi></msub><mo>‚àí</mo><msub><mi>ùê´</mi><mi mathvariant="normal">i</mi></msub></mrow></math>
	<br>
	<math display="block"><mrow><msub><mi>d</mi><mi>ij</mi></msub><mo>=</mo><msup><mrow><mo>‚Äñ</mo><msub><mi>ùê´</mi><mi>ij</mi></msub><mo>‚Äñ</mo></mrow><mn>2</mn></msup></mrow></math>
	<br>
	<math display="block"><mrow><msub><mover><mi>ùê´</mi><mo>^</mo></mover><mi>ij</mi></msub><mo>=</mo><mfrac><msub><mi>ùê´</mi><mi>ij</mi></msub><msub><mi>d</mi><mi>ij</mi></msub></mfrac></mrow></math>
	<br>

<li> Concatenates scalar features to form message input:</li>

<br>

	<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
			<msub><mi>m</mi><mi>ij</mi></msub>
			<mo>=</mo>
			<msub><mi>MLP</mi><mi>msg</mi></msub><mo>(</mo><mrow><msubsup><mi>x</mi><mi>i</mi><mrow><mo>(</mo><mi>l</mi><mo>)</mo></mrow></msubsup></mrow>
			<mo>,</mo>
			<mrow><msubsup><mi>x</mi><mi>j</mi><mrow><mo>(</mo><mi>l</mi><mo>)</mo></mrow></msubsup></mrow>
			<mo>,</mo>
			<mrow><msub><mi>d</mi><mi>ij</mi></msub></mrow>
			<mo>)</mo>
		</math>
		<br>

<li> Splits the output of the message MLP into vector gates and scalar messages:</li>

<br>

		<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
		<mo>(</mo><mrow><msub><mi>g</mi><mi>ij</mi></msub><mo>,</mo><msub><mi>s</mi><mi>ij</mi></msub></mrow><mo>)</mo>
		<mo>=</mo>
		<mo>split</mo><mo>(</mo><msub><mi>m</mi><mi>ij</mi></msub><mo>)</mo>
	</math>
	<br>

<li> Calculates the vector message:</li>

<br>

		<math display="block"><mrow><msubsup><mi>m</mi><mi>ij</mi><mtext>vec</mtext></msubsup><mo>=</mo><msub><mover><mi>ùê´</mi><mo>^</mo></mover><mi>ij</mi></msub><mo>‚äó</mo><msub><mi>g</mi><mi>ij</mi></msub></mrow></math>
	<br>

<li> Aggregates the messages:</li>

<br>

		<math display="block"><mrow><msub><mi>agg</mi><mi>x</mi></msub><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><munder><mo movablelimits="false">‚àë</mo><mrow><mi>j</mi><mo>‚â†</mo><mi>i</mi></mrow></munder></mrow><msub><mi>s</mi><mi>ij</mi></msub></mrow></math>
	<br>
		<math display="block"><mrow><msub><mi>agg</mi><mi>ùêØ</mi></msub><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><munder><mo movablelimits="false">‚àë</mo><mrow><mi>j</mi><mo>‚â†</mo><mi>i</mi></mrow></munder></mrow><msub><mi>m</mi><mi>ij</mi></msub></mrow></math>
	<br>

<li>Mixes the hidden channels of the vector features (applies a single linear layer):</li>

<br>

	<math display="block"><mrow><msubsup><mi>ùêØ</mi><mi mathvariant="normal">i</mi><mtext>mix</mtext></msubsup><mo>=</mo><msubsup><mi>ùêØ</mi><mi mathvariant="normal">i</mi><mrow><mo>(</mo><mi>l</mi><mo>)</mo></mrow></msubsup><msub><mi>W</mi><mtext>mix</mtext></msub></mrow></math>
	<br>

<li> And finally applies the updates:</li>

<br>

	<math display="block"><mrow><msubsup><mi>x</mi><mi mathvariant="normal">i</mi><mrow><mo>(</mo><mi>l</mi><mo>+</mo><mn>1</mn><mo>)</mo></mrow></msubsup><mo>=</mo><msubsup><mi>x</mi><mi mathvariant="normal">i</mi><mrow><mo>(</mo><mi>l</mi><mo>)</mo></mrow></msubsup><mo>+</mo><msub><mi>MLP</mi><mtext>upd</mtext></msub><mrow><mo>(</mo><msubsup><mi>x</mi><mi mathvariant="normal">i</mi><mrow><mo>(</mo><mi>l</mi><mo>)</mo></mrow></msubsup><mo>,</mo><msub><mi>agg</mi><mi>x</mi></msub><mo>(</mo><mi>i</mi><mo>)</mo><mo>)</mo></mrow></mrow></math>
	<br>
	<math display="block"><mrow><msubsup><mi>ùêØ</mi><mi mathvariant="normal">i</mi><mrow><mo>(</mo><mi>l</mi><mo>+</mo><mn>1</mn><mo>)</mo></mrow></msubsup><mo>=</mo><msubsup><mi>ùêØ</mi><mi mathvariant="normal">i</mi><mrow><mo>(</mo><mi>l</mi><mo>)</mo></mrow></msubsup><mo>+</mo><msub><mi>agg</mi><mi>ùêØ</mi></msub><mo>(</mo><mi>i</mi><mo>)</mo><mo>+</mo><msubsup><mi>ùêØ</mi><mi mathvariant="normal">i</mi><mtext>mix</mtext></msubsup></mrow></math>
	</ol>

<p>
The equivariance of this model is guaranteed by the use of relative position vectors <math><msub><mover><mi>ùê´</mi><mo>^</mo></mover><mi>ij</mi></msub></math>.
Because these vectors depend only on the relative positions of atoms, they rotate consistently with any transformation from SO(3).
</p>

<p>
Strawberry, however, uses a slightly different layer architecture. As input, instead of zero initialization, as is used
in Chocolate, information about the molecular geometry is given through using atomic position vectors as the vector
features of the first layer:
</p>
<br>
<math display="block"><mrow><msubsup><mi>ùêØ</mi><mi mathvariant="normal">i</mi><mrow><mo>(</mo><mn>0</mn><mo>)</mo></mrow></msubsup><mo>=</mo><msub><mi>ùê´</mi><mi mathvariant="normal">i</mi></msub><mo>‚äó</mo><msub><mn>ùüè</mn><mi>F</mi></msub></mrow></math>
<br>

<p>
Where, as before, <math><mi>F</mi></math> is the hidden dimension. A single layer of Strawberry:
</p>

<ol>

<li>Computes a "pseudo-position" and "pseudo-distance" from the current vector features:</li>

	<br>

	<math display="block"><mrow><msubsup><mover><mi>ùê´</mi><mo>~</mo></mover><mi mathvariant="normal">i</mi><mrow><mo>(</mo><mi>l</mi><mo>)</mo></mrow></msubsup><mo>=</mo><mfrac><mn>1</mn><mi>F</mi></mfrac><mrow><munderover><mo movablelimits="false">‚àë</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>F</mi></munderover></mrow><msubsup><mi>ùêØ</mi><mrow><mi mathvariant="normal">i</mi><mo>,</mo><mo>:</mo><mo>,</mo><mi mathvariant="normal">k</mi></mrow><mrow><mo>(</mo><mi>l</mi><mo>)</mo></mrow></msubsup></mrow></math>

	<br>

	<math display="block"><mrow><msubsup><mover><mi>ùê´</mi><mo>~</mo></mover><mi>ij</mi><mrow><mo>(</mo><mi>l</mi><mo>)</mo></mrow></msubsup><mo>=</mo><msubsup><mover><mi>ùê´</mi><mo>~</mo></mover><mi mathvariant="normal">j</mi><mrow><mo>(</mo><mi>l</mi><mo>)</mo></mrow></msubsup><mo>‚àí</mo><msubsup><mover><mi>ùê´</mi><mo>~</mo></mover><mi mathvariant="normal">i</mi><mrow><mo>(</mo><mi>l</mi><mo>)</mo></mrow></msubsup></mrow></math>

	<br>

	<math display="block"><mrow><msubsup><mover><mi>d</mi><mo>~</mo></mover><mi>ij</mi><mrow><mo>(</mo><mi>l</mi><mo>)</mo></mrow></msubsup><mo>=</mo><msup><mrow><mo>‚Äñ</mo><msubsup><mover><mi>ùê´</mi><mo>~</mo></mover><mi>ij</mi><mrow><mo>(</mo><mi>l</mi><mo>)</mo></mrow></msubsup><mo>‚Äñ</mo></mrow><mn>2</mn></msup></mrow></math>

	<br>

<li> Creates a flattened representation <math><msubsup><mover><mi>ùêØ</mi><mo>~</mo></mover><mi mathvariant="normal">i</mi><mrow><mo>(</mo><mi>l</mi><mo>)</mo></mrow></msubsup></math> of the vector features and concatenates it with the scalar features:</li>

<br>

	<math display="block"><mrow><msubsup><mover><mi>ùêØ</mi><mo>~</mo></mover><mi mathvariant="normal">i</mi><mrow><mo>(</mo><mi>l</mi><mo>)</mo></mrow></msubsup><mo>=</mo><mi>flatten</mi><mrow><mo>(</mo><msubsup><mi>ùêØ</mi><mi mathvariant="normal">i</mi><mrow><mo>(</mo><mi>l</mi><mo>)</mo></mrow></msubsup><mo>)</mo></mrow></mrow></math>

	<br>

	<math display="block"><mrow><msubsup><mi>u</mi><mi mathvariant="normal">i</mi><mrow><mo>(</mo><mi>l</mi><mo>)</mo></mrow></msubsup><mo>=</mo><mrow><mo>(</mo><msubsup><mi>x</mi><mi mathvariant="normal">i</mi><mrow><mo>(</mo><mi>l</mi><mo>)</mo></mrow></msubsup><mo>,</mo><msubsup><mover><mi>ùêØ</mi><mo>~</mo></mover><mi mathvariant="normal">i</mi><mrow><mo>(</mo><mi>l</mi><mo>)</mo></mrow></msubsup><mo>)</mo></mrow></mrow></math>

	<br>

<li> Computes messages with an MLP:</li>

<br>

<math display="block"><mrow><msub><mi>m</mi><mi>ij</mi></msub><mo>=</mo><msub><mi>MLP</mi><mtext>msg</mtext></msub><mrow><mo>(</mo><msubsup><mi>u</mi><mi mathvariant="normal">i</mi><mrow><mo>(</mo><mi>l</mi><mo>)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>u</mi><mi mathvariant="normal">j</mi><mrow><mo>(</mo><mi>l</mi><mo>)</mo></mrow></msubsup><mo>,</mo><msubsup><mover><mi>d</mi><mo>~</mo></mover><mi>ij</mi><mrow><mo>(</mo><mi>l</mi><mo>)</mo></mrow></msubsup><mo>)</mo></mrow></mrow></math>

<br>

<li> Splits the output of the message MLP into vector gates and scalar messages:</li>

<br>

<math display="block"><mrow><mrow><mo>(</mo><msubsup><mi>m</mi><mi>ij</mi><mtext>scalar</mtext></msubsup><mo>,</mo><msubsup><mi>m</mi><mi>ij</mi><mtext>flat</mtext></msubsup><mo>)</mo></mrow><mo>=</mo><mi>split</mi><mrow><mo>(</mo><msub><mi>m</mi><mi>ij</mi></msub><mo>)</mo></mrow></mrow></math>

	<br>

<li> Reshapes the vector message:</li>

<br>

<math display="block"><mrow><msubsup><mi>m</mi><mi>ij</mi><mtext>vec</mtext></msubsup><mo>=</mo><mi>reshape</mi><mrow><mo>(</mo><msubsup><mi>m</mi><mi>ij</mi><mtext>flat</mtext></msubsup><mo>)</mo></mrow></mrow></math>

	<br>

<li> Aggregates the messages:</li>

<br>

	<math display="block"><mrow><msub><mi>agg</mi><mi>x</mi></msub><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><munder><mo movablelimits="false">‚àë</mo><mrow><mi>j</mi><mo>‚â†</mo><mi>i</mi></mrow></munder></mrow><msubsup><mi>m</mi><mi>ij</mi><mtext>scalar</mtext></msubsup></mrow></math>

	<br>

	<math display="block"><mrow><msub><mi>agg</mi><mi>ùêØ</mi></msub><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><munder><mo movablelimits="false">‚àë</mo><mrow><mi>j</mi><mo>‚â†</mo><mi>i</mi></mrow></munder></mrow><msubsup><mi>m</mi><mi>ij</mi><mtext>vec</mtext></msubsup></mrow></math>

	<br>

<li>Mixes the hidden channels of the vector features (applies a single linear layer):</li>

<br>

	<math display="block"><mrow><msubsup><mi>ùêØ</mi><mi mathvariant="normal">i</mi><mtext>mix</mtext></msubsup><mo>=</mo><msubsup><mi>ùêØ</mi><mi mathvariant="normal">i</mi><mrow><mo>(</mo><mi>l</mi><mo>)</mo></mrow></msubsup><msub><mi>W</mi><mtext>mix</mtext></msub></mrow></math>

	<br>

<li> And finally applies the updates:</li>

<br>

	<math display="block"><mrow><msubsup><mi>x</mi><mi mathvariant="normal">i</mi><mrow><mo>(</mo><mi>l</mi><mo>+</mo><mn>1</mn><mo>)</mo></mrow></msubsup><mo>=</mo><msubsup><mi>x</mi><mi mathvariant="normal">i</mi><mrow><mo>(</mo><mi>l</mi><mo>)</mo></mrow></msubsup><mo>+</mo><msub><mi>MLP</mi><mtext>upd</mtext></msub><mrow><mo>(</mo><msubsup><mi>x</mi><mi mathvariant="normal">i</mi><mrow><mo>(</mo><mi>l</mi><mo>)</mo></mrow></msubsup><mo>,</mo><msub><mi>agg</mi><mi>x</mi></msub><mo>(</mo><mi>i</mi><mo>)</mo><mo>)</mo></mrow></mrow></math>

	<br>

	<math display="block"><mrow><msubsup><mi>ùêØ</mi><mi mathvariant="normal">i</mi><mrow><mo>(</mo><mi>l</mi><mo>+</mo><mn>1</mn><mo>)</mo></mrow></msubsup><mo>=</mo><msubsup><mi>ùêØ</mi><mi mathvariant="normal">i</mi><mrow><mo>(</mo><mi>l</mi><mo>)</mo></mrow></msupsup><mo>+</mo><msub><mi>agg</mi><mi>ùêØ</mi></msub><mo>(</mo><mi>i</mi><mo>)</mo><mo>+</mo><msubsup><mi>ùêØ</mi><mi mathvariant="normal">i</mi><mtext>mix</mtext></msubsup></mrow></math>

	</ol>

	<h3>Directly penalizing deviations from equivariance</h3>

	<p>
	To encourange Strawberry to be equivariant, we had the model output a list of the intermediate vector features as well
	as the final vector dipole prediction. During training with data augmentation, we examined the effect of adding an
	extra loss term that penalized non-equivariant representations of intermediate vector features in Strawberry:
	</p>

	<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
		<!-- You can insert your explicit loss formula here later -->
	</math>

	<p>
	Adding this loss term is tantamount to adding an extra objective. To avoid the model getting stuck in a local minimum
	by predicting a trivial dipole (the zero vector) that perfectly satisfies equivariance, we scale this additional loss
	term by a hyperparameter LAMBDA. For future implementations of such a layerwise loss, we will perform a
	hyperparameter search to find optimal values of lambda.
	</p>

			</div>
			<div class="margin-right-block">
						Margin note that clarifies some detail #main-content-block for intro section.
		    </div>
		</div>

		<!-- RESULTS SECTION (anchor before the "Does Strawberry learn..." block) -->
		<div class="content-margin-container" id="results">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
					<h1>Results: Does Strawberry learn to be equivariant?</h1>
					<p>
						Non-equivariant models with sufficient expressivity and information about geometry are known to be
						capable of learning equivariance. In this section, we empirically evaluate whether Strawberry,
						trained with rotational data augmentation, approximates SE(3) equivariance in practice.
					</p>

					<!-- original content kept -->
					<p>
					Non-equivariant models with sufficient expressivity and information about geometry are known to be
					capable of learning equivariance.
					</p>

				<math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
              <mrow>
                <mrow>
                  <mo>&#x2202;</mo>
                  <mi>y</mi>
                </mrow>
                <mo>/</mo>
                <mrow>
                  <mo>&#x2202;</mo>
                  <mi>x</mi>
                </mrow>
              </mrow>
              <mo>=</mo>
              <mi>x</mi>
            </math>
          <br>
          It's probably best to ask an LLM to help do the web
          formatting for math. You can tell it "convert this latex equation into MathML: $$\frac{\partial dy}{\partial dx} = x$$"
          But it took me a few tries. So, if you get frustrated, you can embed an image of the equation, or use other packages for
          rendering equations on webpages.

		    </div>
		    <div class="margin-right-block" style="transform: translate(0%, -100%);"> <!-- you can move the margin notes up and down with translate -->
          Interestingly, Plato also asked if X does Y, in <a href="#ref_1">[1]</a>.
		    </div>
		</div>

		<!-- EVALUATION SECTION -->
		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
					<a id="evaluation"></a>
					<h1>Evaluation: Do layerwise losses help?</h1>
					<p>
						Implementing layerwise losses adds a hard equivariance objective for the non-equivariant model.
						Data augmentation alone provides a soft objective in favor of equivariance. Here we analyze Strawberry‚Äôs
						performance with and without the additional layerwise equivariance penalties.
					</p>

          <math display="block" xmlns="http://www.w3.org/1998/Math/MathML">
              <mrow>
                <mrow>
                  <mo>&#x2202;</mo>
                  <mi>y</mi>
                </mrow>
                <mo>/</mo>
                <mrow>
                  <mo>&#x2202;</mo>
                  <mi>x</mi>
                </mrow>
              </mrow>
              <mo>=</mo>
              <mi>x</mi>
            </math>
          <br>
          It's probably best to ask an LLM to help do the web
          formatting for math. You can tell it "convert this latex equation into MathML: $$\frac{\partial dy}{\partial dx} = x$$"
          But it took me a few tries. So, if you get frustrated, you can embed an image of the equation, or use other packages for
          rendering equations on webpages.

		    </div>
		    <div class="margin-right-block" style="transform: translate(0%, -100%);"> <!-- you can move the margin notes up and down with translate -->
          Interestingly, Plato also asked if X does Y, in <a href="#ref_1">[1]</a>.
		    </div>
		</div>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
            <h1>Another section</h1>
            In this section we embed a video:
						<video class='my-video' loop autoplay muted style="width: 725px">
								<source src="./images/mtsh.mp4" type="video/mp4">
						</video>
		    </div>
		    <div class="margin-right-block">
					A caption for the video could go here.
		    </div>
		</div>

		<!-- CONCLUSION SECTION -->
		<div class="content-margin-container" id="conclusion">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>Conclusion</h1>
						<p>
							We compared three models‚ÄîVanilla, Strawberry, and Chocolate‚Äîto investigate whether a
							non-equivariant GNN trained with rotational augmentation can approximate the performance of a
							built-in equivariant architecture on vector-valued properties like dipole moments. Our methods
							and experiments focus on the trade-offs between inductive bias (equivariance by design) and
							computational efficiency (equivariance learned through data and auxiliary losses).
						</p>
						<p>
							In the full report, this section can summarize quantitative results, highlight whether the central
							hypothesis is supported, and connect back to the broader implications for molecular modeling and
							equivariant deep learning.
						</p>
		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>

		<div class="content-margin-container" id="implications_and_limitations">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>Implications and limitations</h1>
						Let's end with some discussion of the implications and limitations.
		    </div>
		    <div class="margin-right-block">
		    </div>
		</div>

		<div class="content-margin-container" id="citations">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<div class='citation' id="references" style="height:auto"><br>
							<span style="font-size:16px">References:</span><br><br>
							<a id="ref_1"></a>[1] <a href="https://en.wikipedia.org/wiki/Allegory_of_the_cave">Allegory of the Cave</a>, Plato, c. 375 BC<br><br>
							<a id="ref_2"></a>[2] <a href="">A Human-Level AGI</a>, OpenAI, 2025<br><br>
						</div>
		    </div>
		    <div class="margin-right-block">
            <!-- margin notes for reference block here -->
		    </div>
		</div>

	</body>

</html>